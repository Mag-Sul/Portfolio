{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Minimal\n"
     ]
    }
   ],
   "source": [
    "## helpful packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "## nltk imports\n",
    "import nltk\n",
    "### uncomment and run these lines if you haven't downloaded relevant nltk add-ons yet\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## spacy imports\n",
    "import spacy\n",
    "### uncomment and run the below line if you haven't loaded the en_core_web_sm library yet\n",
    "#! python3 -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "## vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## sentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## lda\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## repeated printouts and wide-format text\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "## suppresses the traceback of an error\n",
    "%xmode Minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analysis of Department of Justice (DOJ) press releases (50 points total)\n",
    "\n",
    "- For background:\n",
    "\n",
    "    - DOJ is the federal law enforcement agency responsible for federal prosecutions; this contrasts with the local prosecutions in the Cook County dataset we analyzed earlier. Here's a short explainer on which crimes get prosecuted federally versus locally: https://www.criminaldefenselawyer.com/resources/criminal-defense/federal-crime/state-vs-federal-crimes.htm#:~:text=Federal%20criminal%20prosecutions%20are%20handled,of%20state%20and%20local%20law. \n",
    "    - Here's the Kaggle that contains the data: https://www.kaggle.com/jbencina/department-of-justice-20092018-press-releases \n",
    "    - Here's the code the dataset creator used to scrape those press releases here if you're interested: https://github.com/jbencina/dojreleases\n",
    "    \n",
    "- See here for a codebook: https://docs.google.com/spreadsheets/d/1UopmSvFGrwJvz_c3Plh32Yxkqwff64oS_CcpfATOV8k/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>date</th>\n",
       "      <th>topics_clean</th>\n",
       "      <th>components_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Convicted Bomb Plotter Sentenced to 30 Years</td>\n",
       "      <td>PORTLAND, Oregon. – Mohamed Osman Mohamud, 23, who was convicted in 2013 of attempting to use a weapon of mass destruction (explosives) in connection with a plot to detonate a vehicle bomb at an annual Christmas tree lighting ceremony in Portland, was sentenced today to serve 30 years in prison, followed by a lifetime term of supervised release. Mohamud, a naturalized U.S. citizen from Somalia and former resident of Corvallis, Oregon, was arrested on Nov. 26, 2010, after he attempted to detonate what he believed to be an explosives-laden van that was parked near the tree lighting ceremony in Portland.  The arrest was the culmination of a long-term undercover operation, during which Mohamud was monitored closely for months as his bomb plot developed.  The device was in fact inert, and the public was never in danger from the device. At sentencing, United States District Court Judge Garr M. King, who presided over Mohamed’s 14-day trial, said “the intended crime was horrific,” and that the defendant, even though he was presented with options by undercover FBI employees, “never once expressed a change of heart.”  King further noted that the Christmas tree ceremony was attended by up to 10,000 people, and that the defendant “wanted everyone to leave either dead or injured.”  King said his sentence was necessary in view of the seriousness of the crime and to serve as deterrence to others who might consider similar acts.     “With today’s sentencing, Mohamed Osman Mohamud is being held accountable for his attempted use of what he believed to be a massive bomb to attack innocent civilians attending a public Christmas tree lighting ceremony in Portland,” said John P. Carlin, Assistant Attorney General for National Security.  “The evidence clearly indicated that Mohamud was intent on killing as many people as possible with his attack.  Fortunately, law enforcement was able to identify him as a threat, insert themselves in the place of a terrorist that Mohamud was trying to contact, and thwart Mohamud’s efforts to conduct an attack on our soil.  This case highlights how the use of undercover operations against would-be terrorists allows us to engage and disrupt those who wish to commit horrific acts of violence against the innocent public.  The many agents, analysts, and prosecutors who have worked on this case deserve great credit for their roles in protecting Portland from the threat posed by this defendant and ensuring that he was brought to justice.” “This trial provided a rare glimpse into the techniques Al Qaeda employs to radicalize home-grown extremists,” said Amanda Marshall, U.S. Attorney for the District of Oregon.  “With the sentencing today, the court has held this defendant accountable.   I thank the dedicated professionals in the law enforcement and intelligence communities who were responsible for this successful outcome.  I look forward to our continued work with Muslim communities in Oregon who are committed to ensuring that all young people are safe from extremists who seek to radicalize others to engage in violence.”  According to the trial evidence, in February 2009, Mohamud began communicating via e-mail with Samir Khan, a now-deceased al Qaeda terrorist who published Jihad Recollections, an online magazine that advocated violent jihad, and who also published Inspire, the official magazine of al-Qaeda in the Arabian Peninsula.  Between February and August 2009, Mohamed exchanged approximately 150 emails with Khan.  Mohamud wrote several articles for Jihad Recollections that were published under assumed names. In August 2009, Mohamud was in email contact with Amro Al-Ali, a Saudi national who was in Yemen at the time and is today in custody in Saudi Arabia for terrorism offenses.  Al-Ali sent Mohamud detailed e-mails designed to facilitate Mohamud’s travel to Yemen to train for violent jihad.  In December 2009, while Al-Ali was in the northwest frontier province of Pakistan, Mohamud and Al-Ali discussed the possibility of Mohamud traveling to Pakistan to join Al-Ali in terrorist activities. Mohamud responded to Al-Ali in an e-mail: “yes, that would be wonderful, just tell me what I need to do.”  Al-Ali referred Mohamud to a second associate overseas and provided Mohamud with a name and email address to facilitate the process. In the following months, Mohamud made several unsuccessful attempts to contact Al-Ali’s associate.  Ultimately, an FBI undercover operative contacted Mohamud via email under the guise of being an associate of Al-Ali’s.  Mohamud and the FBI undercover operative agreed to meet in Portland in July 2010.  At the meeting, Mohamud told the FBI undercover operative he had written articles that were published in Jihad Recollections.  Mohamud also said that he wanted to become “operational.”  Asked what he meant by “operational,” Mohamud said he wanted to put an explosion together, but needed help. According to evidence presented at trial, at a meeting in August 2010, Mohamud told undercover FBI operatives he had been thinking of committing violent jihad since the age of 15.  Mohamud then told the undercover FBI operatives that he had identified a potential target for a bomb: the annual Christmas tree lighting ceremony in Portland’s Pioneer Courthouse Square on Nov. 26, 2010.  The undercover FBI operatives cautioned Mohamud several times about the seriousness of this plan, noting there would be many people at the event, including children, and emphasized that Mohamud could abandon his attack plans at any time with no shame.  Mohamud indicated the deaths would be justified and that he would not mind carrying out a suicide attack on the crowd. According to evidence presented at trial, in the ensuing months Mohamud continued to express his interest in carrying out the attack and worked on logistics.  On Nov. 4, 2010, Mohamud and the undercover FBI operatives traveled to a remote location in Lincoln County, Oregon, where they detonated a bomb concealed in a backpack as a trial run for the upcoming attack.  During the drive back to Corvallis, Mohamud was asked if was capable looking at all the bodies of those who would be killed during the explosion.  In response, Mohamud noted, “I want whoever is attending that event to be, to leave either dead or injured.”  Mohamud later recorded a video of himself, with the assistance of the undercover FBI operatives, in which he read a statement that offered his rationale for his bomb attack.  On Nov. 18, 2010, undercover FBI operatives picked up Mohamud to travel to Portland to finalize the details of the attack.  On Nov. 26, 2010, just hours before the planned attack, Mohamud examined the 1,800 pound bomb in the van and remarked that it was “beautiful.”  Later that day, Mohamud was arrested after he attempted to remotely detonate the inert vehicle bomb rked near the Christmas tree lighting ceremony This case was investigated by the FBI, with assistance from the Oregon State Police, the Corvallis Police Department, the Lincoln County Sheriff’s Office and the Portland Police Bureau.  The prosecution was handled by Assistant U.S. Attorneys Ethan D. Knight and Pamala Holsinger from the U.S. Attorney’s Office for the District of Oregon.  Trial Attorney Jolie F. Zimmerman, from the Counterterrorism Section of the Justice Department’s National Security Division, assisted. # # # 14-1077</td>\n",
       "      <td>2014-10-01T00:00:00-04:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>National Security Division (NSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-919</td>\n",
       "      <td>$1 Million in Restitution Payments Announced to Preserve North Carolina Wetlands</td>\n",
       "      <td>WASHINGTON – North Carolina’s Waccamaw River watershed will benefit from a $1 million restitution order from a federal court, funding environmental projects to acquire and preserve wetlands in an area damaged by illegal releases of wastewater from a corporate hog farm, announced Ignacia S. Moreno, Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division; U.S. Attorney for the Eastern District of North Carolina Thomas G. Walker; Director Greg McLeod from the North Carolina State Bureau of Investigation; and Camilla M. Herlevich, Executive Director of the North Carolina Coastal Land Trust.   Freedman Farms Inc. was sentenced in February 2012 to five years of probation and ordered to pay $1.5 million in fines, restitution and community service payments for violating the Clean Water Act when it discharged hog waste into a stream that leads to the Waccamaw River.  William B. Freedman, president of Freedman Farms, was sentenced to six months in prison to be followed by six months of home confinement.  Freedman Farms also is required to implement a comprehensive environmental compliance program and institute an annual training program.   In an order issued on April 19, 2012, the court ordered that the defendants would be responsible for restitution of $1 million in the form of five annual payments starting in January 2013, which the court will direct to the North Carolina Coastal Land Trust (NCCLT).  The NCCLT plans to use the money to acquire and conserve land along streams in the Waccamaw watershed.  The court also directed a $75,000 community service payment to the Southern Environmental Enforcement Network, an organization dedicated to environmental law enforcement training and information sharing in the region.    “The resolution of the case against Freedman Farms demonstrates the commitment of the Department of Justice to enforcing the Clean Water Act to ensure the protection of human health and the environment,” said Assistant Attorney General Moreno.  “The court-ordered restitution in this case will conserve wetlands for the benefit of the people of North Carolina.  By enforcing the nation’s environmental laws, we will continue to ensure that concentrated animal feeding operations (CAFOs) operate without threatening our drinking water, the health of our communities and the environment.”   “This office is committed to doing our part to hold accountable those who commit crimes against our environment, which can cause serious health problems to residents and damage the environment that makes North Carolina such a beautiful place to live and visit,” said U.S. Attorney Walker.   “This case shows what we can accomplish when our SBI agents work closely with their local, state and federal partners to investigate environmental crimes and hold the polluters accountable,” said Director McLeod.  “We’ll continue our efforts to fight illegal pollution that damages our water and puts the public’s health at risk.”    “The Waccamaw is unique and wild,” said Director Herlevich of the North Carolina Coastal Land Trust. “Its watershed includes some of the most extensive cypress gum swamps in the state, and its headwaters at Lake Waccamaw contain fish that are found nowhere else on Earth.  We appreciate the trust of the court and the U. S. Attorney, and we look forward to using these funds for conservation projects in a river system that is one of our top conservation priorities.”   According to evidence presented in court, in December 2007 Freedman Farms discharged hog waste into Browder’s Branch, a tributary to the Waccamaw River that flows through the White Marsh, a large wetlands complex.  Freedman Farms, located in Columbus County, N.C., is in the business of raising hogs for market, and this particular farm had some 4,800 hogs.  The hog waste was supposed to be directed to two lagoons for treatment and disposal.  Instead, hog waste was discharged from Freedman Farms directly into Browder’s Branch.    The Clean Water Act is a federal law that makes it illegal to knowingly or negligently discharge a pollutant into a water of the United States.    The Freedman case was investigated by the U.S. Environmental Protection Agency (EPA) Criminal Investigation Division, the U.S. Army Corps of Engineers and the North Carolina State Bureau of Investigation, with assistance from the EPA Science and Ecosystem Support Division.  The case was prosecuted by Assistant U.S. Attorney J. Gaston B. Williams of the Eastern District of North Carolina and Trial Attorney Mary Dee Carraway of the Environmental Crimes Section of the Justice Department’s Environment and Natural Resources Division.   The North Carolina Coastal Land Trust is celebrating its 20th anniversary of saving special lands in eastern North Carolina. The organization has protected nearly 50,000 acres of lands with scenic, recreational, historic and ecological values. North Carolina Coastal Land Trust has saved streams and wetlands that provide clean water, forests that are havens for wildlife, working farms that provide local food and nature parks that everyone can enjoy.  More information about the Coastal Land Trust is available at www.coastallandtrust.org.</td>\n",
       "      <td>2012-07-25T00:00:00-04:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1002</td>\n",
       "      <td>$1 Million Settlement Reached for Natural Resource Damages at Superfund Site in Massachusetts</td>\n",
       "      <td>BOSTON– A $1-million settlement has been reached for natural resource damages (NRD) at the Blackburn &amp; Union Privileges Superfund Site in Walpole, Mass., the Departments of Justice and Interior (DOI), and the Office of the Massachusetts Attorney General announced today.                The Blackburn &amp; Union Privileges Superfund Site includes 22 acres of contaminated land and water in Walpole. The contamination resulted from the operations of various industrial facilities dating back to the 19th century that exposed the site to asbestos, arsenic, lead and other hazardous substances.                The private parties involved in the settlement include two former owners and operators of the site, W.R. Grace &amp; Co.– Conn. and Tyco Healthcare Group LP, as well as the current owners, BIM Investment Corp. and Shaffer Realty Nominee Trust.               From about 1915 to 1936, a predecessor of W.R. Grace manufactured asbestos brake linings and clutch linings on a large portion of the property. From 1946 to about 1983, a predecessor of Tyco Healthcare operated a cotton fabric manufacturing business, which used caustic solutions, on a portion of the property.               In a 2010 settlement with U.S. Environmental Protection Agency (EPA), the four private parties agreed to perform a remedial action to clean up the site at an estimated cost of $13 million. The consent decree lodged today resolves both state and federal NRD liability claims; it requires the parties to pay $1,094,169.56 to the state and federal natural resource trustees, the Massachusetts Executive Office of Energy and Environmental Affairs (EEA) and DOI, for injuries to ecological resources including groundwater and wetlands, which provide habitat for waterfowl and wading birds, including black ducks and great blue herons.  The trustees will use the settlement funds for natural resource restoration projects in the area.               “This settlement demonstrates our commitment to recovering damages from the parties responsible for injury to natural resources, in partnership with state trustees,” said Bruce Gelber, Acting Deputy Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division.               “The citizens of Walpole have had to live with the environmental impact of this contamination for many years,” Attorney General Martha Coakley said. “We are pleased that today’s agreement will not only require the responsible parties to reimburse taxpayer dollars, but will also provide funding to begin restoring or replacing the wetland and other natural resources.”                 The consent decree was lodged in the U.S. District Court for Massachusetts.     A portion of the funds, $300,000, will be distributed to the EEA-sponsored groundwater restoration projects; $575,000 will be used for ecological restoration projects jointly sponsored by EEA and the U.S. Fish and Wildlife Service (FWS).               In addition, $125,000 will go for projects jointly sponsored by EEA and FWS that achieve both ecological and groundwater restoration; $57,491.34 will be allocated for reimbursement for the FWS’s assessment costs; and $36,678.22 will be distributed as reimbursement for the commonwealth’s assessment costs.       “This settlement provides the means for a range of projects designed to compensate the public for decades of groundwater and other ecological damage at this site.  I encourage local citizens and organizations to become engaged in the public process that will take place as we solicit, take comment on, and choose these projects in the months ahead,” said Energy and Environmental Affairs Secretary Richard K. Sullivan Jr., who serves as the Commonwealth’s Natural Resources Damages trustee.       “This settlement will help restore habitat for fish and wildlife in the Neponset River watershed,” said Tom Chapman of the FWS New England Field Office. “We look forward to working with the commonwealth and local stakeholders to implement restoration.”               “More than 100 years-worth of industrial activities at this site caused major environmental contamination to the Neponset River, nearby wetlands and to groundwater below the site,” said Commissioner Kenneth Kimmell of the Massachusetts Department of Environmental Protection (MassDEP), which will staff the Trustee Council for the Commonwealth. “We will ensure that the community and the public will be active participants in the process to use these NRD funds to restore the injured natural resources.”                Under the federal Comprehensive Environmental Response, Compensation and Liability Act, EEA and DOI, acting through the FWS, are the designated state and federal natural resource Trustees for the site. The site has been listed on the EPA’s National Priorities List since 1994.        The consent decree is subject to a public comment period and court approval. A copy of the consent decree and instructions about how to submit comments is available on www.usdoj.gov/enrd/Consent_Decrees.html  .               After the consent decree is approved, EEA and FWS will develop proposed restoration plans to use the settlement funds for restoration projects. The proposed restoration plans will also be made available to the public for review and comment.                Assistant Attorney General Matthew Brock of Massachusetts Attorney General Coakley's Environmental Protection Division handled this matter.  Attorney Jennifer Davis of MassDEP, Attorney Anna Blumkin of EEA and MassDEP’s NRD Coordinator Karen Pelto also worked on this settlement.</td>\n",
       "      <td>2011-08-03T00:00:00-04:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-015</td>\n",
       "      <td>10 Las Vegas Men Indicted \\r\\nfor Falsifying Vehicle Emissions Tests</td>\n",
       "      <td>WASHINGTON—A federal grand jury in Las Vegas today returned indictments against 10 Nevada-certified emissions testers for falsifying vehicle emissions test reports, the Justice Department announced.   Each defendant faces one felony Clean Air Act count for falsifying reports between November 2007 and May 2009. The number of falsifications varied by defendant, with some defendants having falsified approximately 250 records, while others falsified more than double that figure. One defendant is alleged to have falsified over 700 reports.   The individuals indicted include:     Escudero resides in Pahrump, Nev. All other individuals are from Clark County, Nev.    The 10 defendants are alleged to have engaged in a practice known as \"clean scanning\" vehicles. The scheme involved entering the Vehicle Identification Number (VIN) for a vehicle that would not pass the emissions test into the computerized system, then connecting a different vehicle the testers knew would pass the test. These falsifications were allegedly performed for anywhere from $10 to $100 over and above the usual emissions testing fee.    The U.S. Environmental Protection Agency (EPA), under the Clean Air Act, requires the state of Nevada to conduct vehicle emissions testing in certain areas because the areas exceed national standards for carbon monoxide and ozone. Las Vegas is currently required to perform emissions testing.    To obtain a registration renewal, vehicle owners bring the vehicles to a licensed inspection station for testing. The emissions inspector logs into a computer to activate the system by using a unique password issued to the emissions inspector. The emissions inspector manually inputs the vehicle’s VIN to identify the tested vehicle, then connects the vehicle for model year 1996 and later to an onboard diagnostics port connected to an analyzer. The analyzer downloads data from the vehicle’s computer, analyzes the data and provides a \"pass\" or \"fail\" result. The pass or fail result and vehicle identification data are reported on the Vehicle Inspection Report. It is a crime to knowingly alter or conceal any record or other document required to be maintained by the Clean Air Act.     \"Falsifications of vehicle emissions testing, such as those alleged in the indictments unsealed today, are serious matters and we intend to use all of our enforcement tools to stop this harmful practice. These actions undermine a system that is designed to reduce air pollutants including smog and provide better air quality for the citizens of Nevada,\" said Ignacia S. Moreno, Assistant Attorney General for the Justice Department’s Environment and Natural Resources Division.    \"The residents of Nevada deserve to know that the vast majority of licensed vehicle emission inspectors are not corrupt and are not circumventing emission testing procedures,\" said U.S. Attorney Bogden. \"These indictments should serve as a clear warning to offenders that the Department of Justice will prosecute you if you make fraudulent statements and reports concerning compliance with the federal Clean Air Act.\"    \"Lying about car emissions means dirtier air, which is especially of concern in areas like Las Vegas that are already experiencing air quality problems,\" said Cynthia Giles, Assistant Administrator for Enforcement and Compliance Assurance at EPA. \"We will take aggressive action to ensure communities have clean air.\"    The maximum penalty for the felony violations contained in the indictments includes up to two years in prison and a fine of up to $250,000.    An indictment is merely an accusation, and a defendant is presumed innocent unless and until proven guilty in a court of law.    The case was investigated by the EPA, Criminal Investigation Division; and the Nevada Department of Motor Vehicles Compliance Enforcement Division. The case is being prosecuted by the U.S. Attorney’s Office for the District of Nevada and the Justice Department’s Environmental Crimes Section.</td>\n",
       "      <td>2010-01-08T00:00:00-05:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-898</td>\n",
       "      <td>$100 Million Settlement Will Speed Cleanup Work at Centredale Manor Superfund Site in North Providence, R.I.</td>\n",
       "      <td>The U.S. Department of Justice, the U.S. Environmental Protection Agency (EPA), and the Rhode Island Department of Environmental Management (RIDEM) announced today that two subsidiaries of Stanley Black &amp; Decker Inc.—Emhart Industries Inc. and Black &amp; Decker Inc.—have agreed to clean up dioxin contaminated sediment and soil at the Centredale Manor Restoration Project Superfund Site in North Providence and Johnston, Rhode Island.  “We are pleased to reach a resolution through collaborative work with the responsible parties, EPA, and other stakeholders,” said Acting Assistant Attorney General Jeffrey H. Wood for the Justice Department's Environment and Natural Resources Division . “Today’s settlement ends protracted litigation and allows for important work to get underway to restore a healthy environment for citizens living in and around the Centredale Manor Site and the Woonasquatucket River.” “This settlement demonstrates the tremendous progress we are achieving working with responsible parties, states, and our federal partners to expedite sites through the entire Superfund remediation process,” said EPA Acting Administrator Andrew Wheeler. “The Centredale Manor Site has been on the National Priorities List for 18 years; we are taking charge and ensuring the Agency makes good on its promise to clean it up for the betterment of the environment and those communities affected.” “Successfully concluding this settlement paves the way for EPA to make good on our commitment to aggressively pursue cleaning up the Centredale Manor Superfund Site,” said EPA New England Regional Administrator Alexandra Dunn. “We are excited to get to work on the cleanup at this site, and get it closer to the goal of being fully utilized by the North Providence and Johnston communities.” “We are pleased that the collective efforts of the State of Rhode Island, EPA, and DOJ in these negotiations have concluded in this major milestone toward the cleanup of the Centredale Manor Restoration Superfund site and are consistent with our long-standing efforts to make the polluter pay,” said RIDEM Director Janet Coit. “The settlement will speed up a remedy that protects public health and the river environment, and moves us closer to the day that we can reclaim recreational uses of this beautiful river resource.” The settlement, which includes cleanup work in the Woonasquatucket River (River) and bordering residential and commercial properties along the River, requires the companies to perform the remedy selected by EPA for the Site in 2012, which is estimated to cost approximately $100 million, and resolves longstanding litigation. The cleanup remedy includes excavation of contaminated sediment and floodplain soil from the Woonasquatucket River, including from adjacent residential properties. Once the cleanup remedy is completed, full access to the Woonasquatucket River should be restored for local citizens. The cleanup will be a step toward the State’s goal of a fishable and swimmable river. The work will also include upgrading caps over contaminated soil in the peninsula area of the Site that currently house two high-rise apartment buildings. The settlement also ensures that the long-term monitoring and maintenance of the site, as directed in the remedy, will be implemented to ensure that public health is protected.  Under the settlement, Emhart and Black &amp; Decker will reimburse EPA for approximately $42 million in past costs incurred at the Site. The companies will also reimburse EPA and the State of Rhode Island for future costs incurred by those agencies in overseeing the work required by the settlement. The settlement will also include payments on behalf of two federal agencies to resolve claims against those agencies. These payments, along with prior settlements related to the Site, will result in a 100 percent recovery for the United States of its past and future response costs related to the Site. Litigation related to the Site has been ongoing for nearly eight years. While the Federal District Court found Black &amp; Decker and Emhart to be liable for their hazardous waste and responsible to conduct the cleanup of the Site, it had also ruled that EPA needed to reconsider certain aspects of that cleanup. EPA appealed the decision requiring it to reconsider aspects of the cleanup. This settlement, once entered by the District Court, will resolve the litigation between the United States, Rhode Island, and Emhart and Black and Decker, allowing the cleanup of the Site to begin. The Site spans a one and a half mile stretch of the Woonasquatucket River and encompasses a nine-acre peninsula, two ponds and a significant forested wetland. From the 1940s to the early 1970s, Emhart’s predecessor operated a chemical manufacturing facility on the peninsula and used a raw material that was contaminated with 2,3,7,8-tetrachlorodibenzo-p-dioxin, a toxic form of dioxin. The Site property was also previously used by a barrel refurbisher. Elevated levels of dioxins and other contaminants have been detected in soil, groundwater, sediment, surface water and fish.  The Site was added to the National Priorities List (NPL) in 2000, and in December 2017, EPA included the Centredale Manor Restoration Project Superfund Site on a list of Superfund sites targeted for immediate and intense attention. Several short-term actions were previously performed at the Site to address immediate threats to the residents and minimize potential erosion and downstream transport of contaminated soil and sediment. This settlement is the latest agreement EPA has reached since the Site was listed on the NPL. Prior agreements addressed the performance and recovery of costs for the past environmental investigations and interim cleanup actions from Emhart, the barrel reconditioning company, the current owners of the peninsula portion of the Site, and other potentially responsible parties. The Consent Decree, lodged in the U.S. District Court of Rhode Island, will be posted in the Federal Register and available for public comment for a period of 30 days. The Consent Decree can be viewed on the Justice Department website: www.justice.gov/enrd/Consent_Decrees.html.  EPA information on the Centredale Manor Superfund Site: www.epa.gov/superfund/centredale.</td>\n",
       "      <td>2018-07-09T00:00:00-04:00</td>\n",
       "      <td>Environment</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0     None   \n",
       "1  12-919    \n",
       "2  11-1002   \n",
       "3   10-015   \n",
       "4   18-898   \n",
       "\n",
       "                                                                                                          title  \\\n",
       "0                                                                  Convicted Bomb Plotter Sentenced to 30 Years   \n",
       "1                              $1 Million in Restitution Payments Announced to Preserve North Carolina Wetlands   \n",
       "2                 $1 Million Settlement Reached for Natural Resource Damages at Superfund Site in Massachusetts   \n",
       "3                                          10 Las Vegas Men Indicted \\r\\nfor Falsifying Vehicle Emissions Tests   \n",
       "4  $100 Million Settlement Will Speed Cleanup Work at Centredale Manor Superfund Site in North Providence, R.I.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          contents  \\\n",
       "0  PORTLAND, Oregon. – Mohamed Osman Mohamud, 23, who was convicted in 2013 of attempting to use a weapon of mass destruction (explosives) in connection with a plot to detonate a vehicle bomb at an annual Christmas tree lighting ceremony in Portland, was sentenced today to serve 30 years in prison, followed by a lifetime term of supervised release. Mohamud, a naturalized U.S. citizen from Somalia and former resident of Corvallis, Oregon, was arrested on Nov. 26, 2010, after he attempted to detonate what he believed to be an explosives-laden van that was parked near the tree lighting ceremony in Portland.  The arrest was the culmination of a long-term undercover operation, during which Mohamud was monitored closely for months as his bomb plot developed.  The device was in fact inert, and the public was never in danger from the device. At sentencing, United States District Court Judge Garr M. King, who presided over Mohamed’s 14-day trial, said “the intended crime was horrific,” and that the defendant, even though he was presented with options by undercover FBI employees, “never once expressed a change of heart.”  King further noted that the Christmas tree ceremony was attended by up to 10,000 people, and that the defendant “wanted everyone to leave either dead or injured.”  King said his sentence was necessary in view of the seriousness of the crime and to serve as deterrence to others who might consider similar acts.     “With today’s sentencing, Mohamed Osman Mohamud is being held accountable for his attempted use of what he believed to be a massive bomb to attack innocent civilians attending a public Christmas tree lighting ceremony in Portland,” said John P. Carlin, Assistant Attorney General for National Security.  “The evidence clearly indicated that Mohamud was intent on killing as many people as possible with his attack.  Fortunately, law enforcement was able to identify him as a threat, insert themselves in the place of a terrorist that Mohamud was trying to contact, and thwart Mohamud’s efforts to conduct an attack on our soil.  This case highlights how the use of undercover operations against would-be terrorists allows us to engage and disrupt those who wish to commit horrific acts of violence against the innocent public.  The many agents, analysts, and prosecutors who have worked on this case deserve great credit for their roles in protecting Portland from the threat posed by this defendant and ensuring that he was brought to justice.” “This trial provided a rare glimpse into the techniques Al Qaeda employs to radicalize home-grown extremists,” said Amanda Marshall, U.S. Attorney for the District of Oregon.  “With the sentencing today, the court has held this defendant accountable.   I thank the dedicated professionals in the law enforcement and intelligence communities who were responsible for this successful outcome.  I look forward to our continued work with Muslim communities in Oregon who are committed to ensuring that all young people are safe from extremists who seek to radicalize others to engage in violence.”  According to the trial evidence, in February 2009, Mohamud began communicating via e-mail with Samir Khan, a now-deceased al Qaeda terrorist who published Jihad Recollections, an online magazine that advocated violent jihad, and who also published Inspire, the official magazine of al-Qaeda in the Arabian Peninsula.  Between February and August 2009, Mohamed exchanged approximately 150 emails with Khan.  Mohamud wrote several articles for Jihad Recollections that were published under assumed names. In August 2009, Mohamud was in email contact with Amro Al-Ali, a Saudi national who was in Yemen at the time and is today in custody in Saudi Arabia for terrorism offenses.  Al-Ali sent Mohamud detailed e-mails designed to facilitate Mohamud’s travel to Yemen to train for violent jihad.  In December 2009, while Al-Ali was in the northwest frontier province of Pakistan, Mohamud and Al-Ali discussed the possibility of Mohamud traveling to Pakistan to join Al-Ali in terrorist activities. Mohamud responded to Al-Ali in an e-mail: “yes, that would be wonderful, just tell me what I need to do.”  Al-Ali referred Mohamud to a second associate overseas and provided Mohamud with a name and email address to facilitate the process. In the following months, Mohamud made several unsuccessful attempts to contact Al-Ali’s associate.  Ultimately, an FBI undercover operative contacted Mohamud via email under the guise of being an associate of Al-Ali’s.  Mohamud and the FBI undercover operative agreed to meet in Portland in July 2010.  At the meeting, Mohamud told the FBI undercover operative he had written articles that were published in Jihad Recollections.  Mohamud also said that he wanted to become “operational.”  Asked what he meant by “operational,” Mohamud said he wanted to put an explosion together, but needed help. According to evidence presented at trial, at a meeting in August 2010, Mohamud told undercover FBI operatives he had been thinking of committing violent jihad since the age of 15.  Mohamud then told the undercover FBI operatives that he had identified a potential target for a bomb: the annual Christmas tree lighting ceremony in Portland’s Pioneer Courthouse Square on Nov. 26, 2010.  The undercover FBI operatives cautioned Mohamud several times about the seriousness of this plan, noting there would be many people at the event, including children, and emphasized that Mohamud could abandon his attack plans at any time with no shame.  Mohamud indicated the deaths would be justified and that he would not mind carrying out a suicide attack on the crowd. According to evidence presented at trial, in the ensuing months Mohamud continued to express his interest in carrying out the attack and worked on logistics.  On Nov. 4, 2010, Mohamud and the undercover FBI operatives traveled to a remote location in Lincoln County, Oregon, where they detonated a bomb concealed in a backpack as a trial run for the upcoming attack.  During the drive back to Corvallis, Mohamud was asked if was capable looking at all the bodies of those who would be killed during the explosion.  In response, Mohamud noted, “I want whoever is attending that event to be, to leave either dead or injured.”  Mohamud later recorded a video of himself, with the assistance of the undercover FBI operatives, in which he read a statement that offered his rationale for his bomb attack.  On Nov. 18, 2010, undercover FBI operatives picked up Mohamud to travel to Portland to finalize the details of the attack.  On Nov. 26, 2010, just hours before the planned attack, Mohamud examined the 1,800 pound bomb in the van and remarked that it was “beautiful.”  Later that day, Mohamud was arrested after he attempted to remotely detonate the inert vehicle bomb rked near the Christmas tree lighting ceremony This case was investigated by the FBI, with assistance from the Oregon State Police, the Corvallis Police Department, the Lincoln County Sheriff’s Office and the Portland Police Bureau.  The prosecution was handled by Assistant U.S. Attorneys Ethan D. Knight and Pamala Holsinger from the U.S. Attorney’s Office for the District of Oregon.  Trial Attorney Jolie F. Zimmerman, from the Counterterrorism Section of the Justice Department’s National Security Division, assisted. # # # 14-1077   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       WASHINGTON – North Carolina’s Waccamaw River watershed will benefit from a $1 million restitution order from a federal court, funding environmental projects to acquire and preserve wetlands in an area damaged by illegal releases of wastewater from a corporate hog farm, announced Ignacia S. Moreno, Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division; U.S. Attorney for the Eastern District of North Carolina Thomas G. Walker; Director Greg McLeod from the North Carolina State Bureau of Investigation; and Camilla M. Herlevich, Executive Director of the North Carolina Coastal Land Trust.   Freedman Farms Inc. was sentenced in February 2012 to five years of probation and ordered to pay $1.5 million in fines, restitution and community service payments for violating the Clean Water Act when it discharged hog waste into a stream that leads to the Waccamaw River.  William B. Freedman, president of Freedman Farms, was sentenced to six months in prison to be followed by six months of home confinement.  Freedman Farms also is required to implement a comprehensive environmental compliance program and institute an annual training program.   In an order issued on April 19, 2012, the court ordered that the defendants would be responsible for restitution of $1 million in the form of five annual payments starting in January 2013, which the court will direct to the North Carolina Coastal Land Trust (NCCLT).  The NCCLT plans to use the money to acquire and conserve land along streams in the Waccamaw watershed.  The court also directed a $75,000 community service payment to the Southern Environmental Enforcement Network, an organization dedicated to environmental law enforcement training and information sharing in the region.    “The resolution of the case against Freedman Farms demonstrates the commitment of the Department of Justice to enforcing the Clean Water Act to ensure the protection of human health and the environment,” said Assistant Attorney General Moreno.  “The court-ordered restitution in this case will conserve wetlands for the benefit of the people of North Carolina.  By enforcing the nation’s environmental laws, we will continue to ensure that concentrated animal feeding operations (CAFOs) operate without threatening our drinking water, the health of our communities and the environment.”   “This office is committed to doing our part to hold accountable those who commit crimes against our environment, which can cause serious health problems to residents and damage the environment that makes North Carolina such a beautiful place to live and visit,” said U.S. Attorney Walker.   “This case shows what we can accomplish when our SBI agents work closely with their local, state and federal partners to investigate environmental crimes and hold the polluters accountable,” said Director McLeod.  “We’ll continue our efforts to fight illegal pollution that damages our water and puts the public’s health at risk.”    “The Waccamaw is unique and wild,” said Director Herlevich of the North Carolina Coastal Land Trust. “Its watershed includes some of the most extensive cypress gum swamps in the state, and its headwaters at Lake Waccamaw contain fish that are found nowhere else on Earth.  We appreciate the trust of the court and the U. S. Attorney, and we look forward to using these funds for conservation projects in a river system that is one of our top conservation priorities.”   According to evidence presented in court, in December 2007 Freedman Farms discharged hog waste into Browder’s Branch, a tributary to the Waccamaw River that flows through the White Marsh, a large wetlands complex.  Freedman Farms, located in Columbus County, N.C., is in the business of raising hogs for market, and this particular farm had some 4,800 hogs.  The hog waste was supposed to be directed to two lagoons for treatment and disposal.  Instead, hog waste was discharged from Freedman Farms directly into Browder’s Branch.    The Clean Water Act is a federal law that makes it illegal to knowingly or negligently discharge a pollutant into a water of the United States.    The Freedman case was investigated by the U.S. Environmental Protection Agency (EPA) Criminal Investigation Division, the U.S. Army Corps of Engineers and the North Carolina State Bureau of Investigation, with assistance from the EPA Science and Ecosystem Support Division.  The case was prosecuted by Assistant U.S. Attorney J. Gaston B. Williams of the Eastern District of North Carolina and Trial Attorney Mary Dee Carraway of the Environmental Crimes Section of the Justice Department’s Environment and Natural Resources Division.   The North Carolina Coastal Land Trust is celebrating its 20th anniversary of saving special lands in eastern North Carolina. The organization has protected nearly 50,000 acres of lands with scenic, recreational, historic and ecological values. North Carolina Coastal Land Trust has saved streams and wetlands that provide clean water, forests that are havens for wildlife, working farms that provide local food and nature parks that everyone can enjoy.  More information about the Coastal Land Trust is available at www.coastallandtrust.org.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        BOSTON– A $1-million settlement has been reached for natural resource damages (NRD) at the Blackburn & Union Privileges Superfund Site in Walpole, Mass., the Departments of Justice and Interior (DOI), and the Office of the Massachusetts Attorney General announced today.                The Blackburn & Union Privileges Superfund Site includes 22 acres of contaminated land and water in Walpole. The contamination resulted from the operations of various industrial facilities dating back to the 19th century that exposed the site to asbestos, arsenic, lead and other hazardous substances.                The private parties involved in the settlement include two former owners and operators of the site, W.R. Grace & Co.– Conn. and Tyco Healthcare Group LP, as well as the current owners, BIM Investment Corp. and Shaffer Realty Nominee Trust.               From about 1915 to 1936, a predecessor of W.R. Grace manufactured asbestos brake linings and clutch linings on a large portion of the property. From 1946 to about 1983, a predecessor of Tyco Healthcare operated a cotton fabric manufacturing business, which used caustic solutions, on a portion of the property.               In a 2010 settlement with U.S. Environmental Protection Agency (EPA), the four private parties agreed to perform a remedial action to clean up the site at an estimated cost of $13 million. The consent decree lodged today resolves both state and federal NRD liability claims; it requires the parties to pay $1,094,169.56 to the state and federal natural resource trustees, the Massachusetts Executive Office of Energy and Environmental Affairs (EEA) and DOI, for injuries to ecological resources including groundwater and wetlands, which provide habitat for waterfowl and wading birds, including black ducks and great blue herons.  The trustees will use the settlement funds for natural resource restoration projects in the area.               “This settlement demonstrates our commitment to recovering damages from the parties responsible for injury to natural resources, in partnership with state trustees,” said Bruce Gelber, Acting Deputy Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division.               “The citizens of Walpole have had to live with the environmental impact of this contamination for many years,” Attorney General Martha Coakley said. “We are pleased that today’s agreement will not only require the responsible parties to reimburse taxpayer dollars, but will also provide funding to begin restoring or replacing the wetland and other natural resources.”                 The consent decree was lodged in the U.S. District Court for Massachusetts.     A portion of the funds, $300,000, will be distributed to the EEA-sponsored groundwater restoration projects; $575,000 will be used for ecological restoration projects jointly sponsored by EEA and the U.S. Fish and Wildlife Service (FWS).               In addition, $125,000 will go for projects jointly sponsored by EEA and FWS that achieve both ecological and groundwater restoration; $57,491.34 will be allocated for reimbursement for the FWS’s assessment costs; and $36,678.22 will be distributed as reimbursement for the commonwealth’s assessment costs.       “This settlement provides the means for a range of projects designed to compensate the public for decades of groundwater and other ecological damage at this site.  I encourage local citizens and organizations to become engaged in the public process that will take place as we solicit, take comment on, and choose these projects in the months ahead,” said Energy and Environmental Affairs Secretary Richard K. Sullivan Jr., who serves as the Commonwealth’s Natural Resources Damages trustee.       “This settlement will help restore habitat for fish and wildlife in the Neponset River watershed,” said Tom Chapman of the FWS New England Field Office. “We look forward to working with the commonwealth and local stakeholders to implement restoration.”               “More than 100 years-worth of industrial activities at this site caused major environmental contamination to the Neponset River, nearby wetlands and to groundwater below the site,” said Commissioner Kenneth Kimmell of the Massachusetts Department of Environmental Protection (MassDEP), which will staff the Trustee Council for the Commonwealth. “We will ensure that the community and the public will be active participants in the process to use these NRD funds to restore the injured natural resources.”                Under the federal Comprehensive Environmental Response, Compensation and Liability Act, EEA and DOI, acting through the FWS, are the designated state and federal natural resource Trustees for the site. The site has been listed on the EPA’s National Priorities List since 1994.        The consent decree is subject to a public comment period and court approval. A copy of the consent decree and instructions about how to submit comments is available on www.usdoj.gov/enrd/Consent_Decrees.html  .               After the consent decree is approved, EEA and FWS will develop proposed restoration plans to use the settlement funds for restoration projects. The proposed restoration plans will also be made available to the public for review and comment.                Assistant Attorney General Matthew Brock of Massachusetts Attorney General Coakley's Environmental Protection Division handled this matter.  Attorney Jennifer Davis of MassDEP, Attorney Anna Blumkin of EEA and MassDEP’s NRD Coordinator Karen Pelto also worked on this settlement.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           WASHINGTON—A federal grand jury in Las Vegas today returned indictments against 10 Nevada-certified emissions testers for falsifying vehicle emissions test reports, the Justice Department announced.   Each defendant faces one felony Clean Air Act count for falsifying reports between November 2007 and May 2009. The number of falsifications varied by defendant, with some defendants having falsified approximately 250 records, while others falsified more than double that figure. One defendant is alleged to have falsified over 700 reports.   The individuals indicted include:     Escudero resides in Pahrump, Nev. All other individuals are from Clark County, Nev.    The 10 defendants are alleged to have engaged in a practice known as \"clean scanning\" vehicles. The scheme involved entering the Vehicle Identification Number (VIN) for a vehicle that would not pass the emissions test into the computerized system, then connecting a different vehicle the testers knew would pass the test. These falsifications were allegedly performed for anywhere from $10 to $100 over and above the usual emissions testing fee.    The U.S. Environmental Protection Agency (EPA), under the Clean Air Act, requires the state of Nevada to conduct vehicle emissions testing in certain areas because the areas exceed national standards for carbon monoxide and ozone. Las Vegas is currently required to perform emissions testing.    To obtain a registration renewal, vehicle owners bring the vehicles to a licensed inspection station for testing. The emissions inspector logs into a computer to activate the system by using a unique password issued to the emissions inspector. The emissions inspector manually inputs the vehicle’s VIN to identify the tested vehicle, then connects the vehicle for model year 1996 and later to an onboard diagnostics port connected to an analyzer. The analyzer downloads data from the vehicle’s computer, analyzes the data and provides a \"pass\" or \"fail\" result. The pass or fail result and vehicle identification data are reported on the Vehicle Inspection Report. It is a crime to knowingly alter or conceal any record or other document required to be maintained by the Clean Air Act.     \"Falsifications of vehicle emissions testing, such as those alleged in the indictments unsealed today, are serious matters and we intend to use all of our enforcement tools to stop this harmful practice. These actions undermine a system that is designed to reduce air pollutants including smog and provide better air quality for the citizens of Nevada,\" said Ignacia S. Moreno, Assistant Attorney General for the Justice Department’s Environment and Natural Resources Division.    \"The residents of Nevada deserve to know that the vast majority of licensed vehicle emission inspectors are not corrupt and are not circumventing emission testing procedures,\" said U.S. Attorney Bogden. \"These indictments should serve as a clear warning to offenders that the Department of Justice will prosecute you if you make fraudulent statements and reports concerning compliance with the federal Clean Air Act.\"    \"Lying about car emissions means dirtier air, which is especially of concern in areas like Las Vegas that are already experiencing air quality problems,\" said Cynthia Giles, Assistant Administrator for Enforcement and Compliance Assurance at EPA. \"We will take aggressive action to ensure communities have clean air.\"    The maximum penalty for the felony violations contained in the indictments includes up to two years in prison and a fine of up to $250,000.    An indictment is merely an accusation, and a defendant is presumed innocent unless and until proven guilty in a court of law.    The case was investigated by the EPA, Criminal Investigation Division; and the Nevada Department of Motor Vehicles Compliance Enforcement Division. The case is being prosecuted by the U.S. Attorney’s Office for the District of Nevada and the Justice Department’s Environmental Crimes Section.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The U.S. Department of Justice, the U.S. Environmental Protection Agency (EPA), and the Rhode Island Department of Environmental Management (RIDEM) announced today that two subsidiaries of Stanley Black & Decker Inc.—Emhart Industries Inc. and Black & Decker Inc.—have agreed to clean up dioxin contaminated sediment and soil at the Centredale Manor Restoration Project Superfund Site in North Providence and Johnston, Rhode Island.  “We are pleased to reach a resolution through collaborative work with the responsible parties, EPA, and other stakeholders,” said Acting Assistant Attorney General Jeffrey H. Wood for the Justice Department's Environment and Natural Resources Division . “Today’s settlement ends protracted litigation and allows for important work to get underway to restore a healthy environment for citizens living in and around the Centredale Manor Site and the Woonasquatucket River.” “This settlement demonstrates the tremendous progress we are achieving working with responsible parties, states, and our federal partners to expedite sites through the entire Superfund remediation process,” said EPA Acting Administrator Andrew Wheeler. “The Centredale Manor Site has been on the National Priorities List for 18 years; we are taking charge and ensuring the Agency makes good on its promise to clean it up for the betterment of the environment and those communities affected.” “Successfully concluding this settlement paves the way for EPA to make good on our commitment to aggressively pursue cleaning up the Centredale Manor Superfund Site,” said EPA New England Regional Administrator Alexandra Dunn. “We are excited to get to work on the cleanup at this site, and get it closer to the goal of being fully utilized by the North Providence and Johnston communities.” “We are pleased that the collective efforts of the State of Rhode Island, EPA, and DOJ in these negotiations have concluded in this major milestone toward the cleanup of the Centredale Manor Restoration Superfund site and are consistent with our long-standing efforts to make the polluter pay,” said RIDEM Director Janet Coit. “The settlement will speed up a remedy that protects public health and the river environment, and moves us closer to the day that we can reclaim recreational uses of this beautiful river resource.” The settlement, which includes cleanup work in the Woonasquatucket River (River) and bordering residential and commercial properties along the River, requires the companies to perform the remedy selected by EPA for the Site in 2012, which is estimated to cost approximately $100 million, and resolves longstanding litigation. The cleanup remedy includes excavation of contaminated sediment and floodplain soil from the Woonasquatucket River, including from adjacent residential properties. Once the cleanup remedy is completed, full access to the Woonasquatucket River should be restored for local citizens. The cleanup will be a step toward the State’s goal of a fishable and swimmable river. The work will also include upgrading caps over contaminated soil in the peninsula area of the Site that currently house two high-rise apartment buildings. The settlement also ensures that the long-term monitoring and maintenance of the site, as directed in the remedy, will be implemented to ensure that public health is protected.  Under the settlement, Emhart and Black & Decker will reimburse EPA for approximately $42 million in past costs incurred at the Site. The companies will also reimburse EPA and the State of Rhode Island for future costs incurred by those agencies in overseeing the work required by the settlement. The settlement will also include payments on behalf of two federal agencies to resolve claims against those agencies. These payments, along with prior settlements related to the Site, will result in a 100 percent recovery for the United States of its past and future response costs related to the Site. Litigation related to the Site has been ongoing for nearly eight years. While the Federal District Court found Black & Decker and Emhart to be liable for their hazardous waste and responsible to conduct the cleanup of the Site, it had also ruled that EPA needed to reconsider certain aspects of that cleanup. EPA appealed the decision requiring it to reconsider aspects of the cleanup. This settlement, once entered by the District Court, will resolve the litigation between the United States, Rhode Island, and Emhart and Black and Decker, allowing the cleanup of the Site to begin. The Site spans a one and a half mile stretch of the Woonasquatucket River and encompasses a nine-acre peninsula, two ponds and a significant forested wetland. From the 1940s to the early 1970s, Emhart’s predecessor operated a chemical manufacturing facility on the peninsula and used a raw material that was contaminated with 2,3,7,8-tetrachlorodibenzo-p-dioxin, a toxic form of dioxin. The Site property was also previously used by a barrel refurbisher. Elevated levels of dioxins and other contaminants have been detected in soil, groundwater, sediment, surface water and fish.  The Site was added to the National Priorities List (NPL) in 2000, and in December 2017, EPA included the Centredale Manor Restoration Project Superfund Site on a list of Superfund sites targeted for immediate and intense attention. Several short-term actions were previously performed at the Site to address immediate threats to the residents and minimize potential erosion and downstream transport of contaminated soil and sediment. This settlement is the latest agreement EPA has reached since the Site was listed on the NPL. Prior agreements addressed the performance and recovery of costs for the past environmental investigations and interim cleanup actions from Emhart, the barrel reconditioning company, the current owners of the peninsula portion of the Site, and other potentially responsible parties. The Consent Decree, lodged in the U.S. District Court of Rhode Island, will be posted in the Federal Register and available for public comment for a period of 30 days. The Consent Decree can be viewed on the Justice Department website: www.justice.gov/enrd/Consent_Decrees.html.  EPA information on the Centredale Manor Superfund Site: www.epa.gov/superfund/centredale.   \n",
       "\n",
       "                        date topics_clean  \\\n",
       "0  2014-10-01T00:00:00-04:00     No topic   \n",
       "1  2012-07-25T00:00:00-04:00     No topic   \n",
       "2  2011-08-03T00:00:00-04:00     No topic   \n",
       "3  2010-01-08T00:00:00-05:00     No topic   \n",
       "4  2018-07-09T00:00:00-04:00  Environment   \n",
       "\n",
       "                             components_clean  \n",
       "0            National Security Division (NSD)  \n",
       "1  Environment and Natural Resources Division  \n",
       "2  Environment and Natural Resources Division  \n",
       "3  Environment and Natural Resources Division  \n",
       "4  Environment and Natural Resources Division  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## first, unzip the combined.json.zip file\n",
    "## then, run this code to load the unzipped json file and convert to a dataframe\n",
    "## and convert some of the attributes from lists to values\n",
    "## make sure to change the pathname if you need to\n",
    "doj = pd.read_json(\"combined.json\", lines = True)\n",
    "\n",
    "## due to json, topics are in a list so remove them and concatenate with ;\n",
    "doj['topics_clean'] = [\"; \".join(topic) \n",
    "                      if len(topic) > 0 else \"No topic\" \n",
    "                      for topic in doj.topics]\n",
    "\n",
    "## similarly with components\n",
    "doj['components_clean'] = [\"; \".join(comp) \n",
    "                           if len(comp) > 0 else \"No component\" \n",
    "                           for comp in doj.components]\n",
    "\n",
    "## drop older columns from data\n",
    "doj = doj[['id', 'title', 'contents', 'date', 'topics_clean', \n",
    "           'components_clean']].copy()\n",
    "\n",
    "doj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tagging and sentiment scoring (16 points)\n",
    "\n",
    "Focus on the following press release: `id` == \"17-1204\" about this pharmaceutical kickback prosecution: https://www.forbes.com/sites/michelatindera/2017/11/16/fentanyl-billionaire-john-kapoor-to-plead-not-guilty-in-opioid-kickback-case/?sh=21b8574d6c6c \n",
    "\n",
    "The `contents` column is the one we're treating as a document. You may need to to convert it from a pandas series to a single string.\n",
    "\n",
    "We'll call the raw string of this press release `pharma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset to one press release \n",
    "subset = doj.contents.loc[doj.id == \"17-1204\"]\n",
    "\n",
    "# convert to a string\n",
    "pharma = subset.to_string()\n",
    "\n",
    "# check this worked\n",
    "type(pharma)\n",
    "#pharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 part of speech tagging (3 points)\n",
    "\n",
    "A. Preprocess the `pharma` press release to remove all punctuation / digits (so can use `.isalpha()` to subset)\n",
    "\n",
    "B. With the preprocessed press release from part A, use the part of speech tagger within nltk to tag all the words in that one press release with their part of speech. \n",
    "\n",
    "C. Using the output from B, extract the adjectives and sort those adjectives from most occurrences to fewest occurrences. Print a dataframe with the 5 most frequent adjectives and their counts in the `pharma` release. See here for a list of the names of adjectives within nltk: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A\n",
    "\n",
    "# remove all punctuation / digits (so can use .isalpha() to subset)\n",
    "\n",
    "# tokenize the string\n",
    "pharm_tokens = word_tokenize(pharma).copy()\n",
    "\n",
    "# check this worked\n",
    "#pharm_tokens\n",
    "#type(pharm_tokens)\n",
    "\n",
    "# use list iteration to subset to only alphabetical tokens\n",
    "processed = [token for token in pharm_tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('founder', 'NN'), ('and', 'CC'), ('majority', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## B\n",
    "\n",
    "# speech tagging\n",
    "tagged = pos_tag(processed)\n",
    "\n",
    "# check this worked\n",
    "tagged[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>former</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>opioid</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>nationwide</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>other</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>addictive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           adj  count\n",
       "19      former      8\n",
       "40      opioid      5\n",
       "38  nationwide      4\n",
       "41       other      3\n",
       "4    addictive      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## C\n",
    "\n",
    "# Using the output from B, extract the adjectives and sort those adjectives from most occurrences to\n",
    "# fewest occurrences. \n",
    "adj = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "adj_list = [one_tok[0] for one_tok in tagged if one_tok[1] in adj]\n",
    "pharm_adj = pd.DataFrame(adj_list, columns = ['adj']).groupby(\"adj\").size().reset_index()\n",
    "pharm_adj.columns = ['adj', \"count\"] \n",
    "pharm_adj_sorted = pharm_adj.sort_values(by = \"count\", ascending = False).copy()\n",
    "\n",
    "#Print a dataframe with the 5 most frequent adjectives and their counts in the pharma release. \n",
    "pharm_adj_sorted.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 named entity recognition (3 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Using the original `pharma` press release (so the one before stripping punctuation/digits), use spaCy to extract all named entities from the press release.\n",
    "\n",
    "B. Print the unique named entities with the tag: `LAW`. Here's some background on what RICO means: https://www.justia.com/criminal/docs/rico/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4909, Insys Therapeutics Inc., today, Fentanyl)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A\n",
    "\n",
    "# extract the named entities \n",
    "spacy_pharma = nlp(pharma)\n",
    "\n",
    "# check this worked\n",
    "#type(spacy_pharma)\n",
    "spacy_pharma.ents[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RICO', 'the Controlled Substances Act', 'RICO']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## B\n",
    "\n",
    "# use list iteration to print the unique named entities with the tag: LAW.\n",
    "law_tags = [token.text for token in spacy_pharma.ents if token.label_ == \"LAW\"]\n",
    "law_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. You want to extract the possible sentence lengths the CEO is facing; pull out the named entities with (1) the label `DATE` and (2) that contain the word year or years. Print these named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last year', 'three years', 'three years']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## C\n",
    "\n",
    "# Pull out the named entities with \n",
    "# (1) the label DATE and \n",
    "# (2) that contain the word year or years. \n",
    "year_pattern = r\"year|years\"     # create a pattern to search for year or years\n",
    "year_tokens = [token.text for token in spacy_pharma.ents if token.label_ == \"DATE\" \n",
    "             and re.search(year_pattern, token.text) is not None]\n",
    "year_tokens  # print matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Parse the pharma string at the sentence level. Note that this involves more than just splitting on each `.`; for full credit, add at least one additional delimiter that marks the end of the sentence.\n",
    "\n",
    "Then, using those sentences, pull and print the original sentences from the press releases where those year lengths are mentioned. Describe in your own words (1 sentence) what length of sentence (prison) and probation (supervised release) the CEO may be facing if convicted after this indictment (if there are multiple lengths mentioned describe the maximum). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4909    The founder and majority owner of Insys Therapeutics Inc., was arrested today and charged with leading a nationwide conspiracy to profit by using bribes and fraud to cause the illegal distribution of a Fentanyl spray intended for cancer patients experiencing breakthrough pain.\\xa0\"More than 20,000 Americans died of synthetic opioid overdoses last year, and millions are addicted to opioids. And yet some medical professionals would rather take advantage of the addicts than try to help them,\" said Attorney General Jeff Sessions. \"This Justice Department will not tolerate this.\\xa0 We will hold accountable anyone – from street dealers to corporate executives -- who illegally contributes to this nationwide epidemic.\\xa0 And under the leadership of President Trump, we are fully committed to defeating this threat to the American people.”John N. Kapoor, 74, of Phoenix, Ariz., a current member of the Board of Directors of Insys, was arrested this morning in Arizona and charged with RICO conspiracy, as well as other felonies, including conspiracy to commit mail and wire fraud and conspiracy to violate the Anti-Kickback Law. Kapoor, the former Executive Chairman of the Board and CEO of Insys, will appear in federal court in Phoenix today.\\xa0 He will appear in U.S. District Court in Boston at a later date.\\xa0The superseding indictment, unsealed today in Boston, also includes additional allegations against several former Insys executives and managers who were initially indicted in December 2016.The superseding indictment charges that Kapoor; Michael L. Babich, 40, of Scottsdale, Ariz., former CEO and President of the company; Alec Burlakoff, 42, of Charlotte, N.C., former Vice President of Sales; Richard M. Simon, 46, of Seal Beach, Calif., former National Director of Sales; former Regional Sales Directors Sunrise Lee, 36, of Bryant City, Mich., and Joseph A. Rowan, 43, of Panama City, Fla.; and former Vice President of Managed Markets, Michael J. Gurry, 53, of Scottsdale, Ariz., conspired to bribe practitioners in various states, many of whom operated pain clinics, in order to get them to prescribe a fentanyl-based pain medication.\\xa0 The medication, called “Subsys,” is a powerful narcotic intended to treat cancer patients suffering intense breakthrough pain.\\xa0 In exchange for bribes and kickbacks, the practitioners wrote large numbers of prescriptions for the patients, most of whom were not diagnosed with cancer.The indictment also alleges that Kapoor and the six former executives conspired to mislead and defraud health insurance providers who were reluctant to approve payment for the drug when it was prescribed for non-cancer patients.\\xa0 They achieved this goal by setting up the “reimbursement unit,” which was dedicated to obtaining prior authorization directly from insurers and pharmacy benefit managers.\\xa0“In the midst of a nationwide opioid epidemic that has reached crisis proportions, Mr. Kapoor and his company stand accused of bribing doctors to overprescribe a potent opioid and committing fraud on insurance companies solely for profit,” said Acting United States Attorney William D. Weinreb. “Today\\'s arrest and charges reflect our ongoing efforts to attack the opioid crisis from all angles. We must hold the industry and its leadership accountable - just as we would the cartels or a street-level drug dealer.”“As alleged, these executives created a corporate culture at Insys that utilized deception and bribery as an acceptable business practice, deceiving patients, and conspiring with doctors and insurers,” said Harold H. Shaw, Special Agent in Charge of the Federal Bureau of Investigation, Boston Field Division. “The allegations of selling a highly addictive opioid cancer pain drug to patients who did not have cancer, make them no better than street-level drug dealers. Today\\'s charges mark an important step in holding pharmaceutical executives responsible for their part in the opioid crisis.\\xa0\\xa0 The FBI will vigorously investigate corrupt organizations with business practices that promote fraud with a total disregard for patient safety.”“These Insys executives allegedly fueled the opioid epidemic by paying doctors to needlessly prescribe an extremely dangerous and addictive form of fentanyl,” said Phillip Coyne, Special Agent in Charge for the Office of Inspector General of the U.S. Department of Health and Human Services.\\xa0 “Corporate executives intent on illegally driving up profits need to be aware they are now squarely in the sights of law enforcement.”“As alleged, Insys executives improperly influenced health care providers to prescribe a powerful opioid for patients who did not need it, and without complying with FDA requirements, thus putting patients at risk and contributing to the current opioid crisis,” said Mark A. McCormack, Special Agent in Charge, FDA Office of Criminal Investigations’ Metro Washington Field Office. “Our office will continue to work with our law enforcement partners to pursue and bring to justice those who threaten the public health.”“Pharmaceutical companies whose products include controlled medications that can lead to addiction and overdose have a special obligation to operate in a trustworthy, transparent manner, because their customers’ health and safety and, indeed, very lives depend on it,” said DEA Special Agent in Charge Michael J. Ferguson.\\xa0 “DEA pledges to work with our law enforcement and regulatory partners nationwide to ensure that rules and regulations under the Controlled Substances Act are followed.”“Today’s arrest is the result of a joint effort to identify, investigate and prosecute individuals who engage in fraudulent activity and endanger patient health,” stated Special Agent in Charge Leigh-Alistair Barzey, Defense Criminal Investigative Service (DCIS) Northeast Field Office.\\xa0 “DCIS will continue to work with the U.S. Attorney’s Office, District of Massachusetts, and our law enforcement partners, to protect U.S. military members, retirees and their dependents and the integrity of TRICARE, the Defense Department’s healthcare system.”“As alleged, John Kapoor and other top executives committed fraud, placing profit before patient safety, to sell a highly potent and addictive opioid.\\xa0 EBSA will take every opportunity to work collaboratively with our law enforcement partners in these important investigations to protect participants in private sector health plans and contribute in fighting the opioid epidemic,” said Susan A. Hensley, Regional Director of the U.S. Department of Labor, Employee Benefits Security Administration, Boston Regional Office.“Once again, the United States Postal Inspection Service is fully committed to protecting our nation’s mail system from criminal misuse,” said Shelly Binkowski, Inspector in Charge of the U.S. Postal Inspection Service. “We are proud to work alongside our law enforcement partners to dismantle high level prescription drug practices which directly contribute to the opioid abuse epidemic.\\xa0 This investigation highlights our commitment to defending our mail system from illegal misuse and ensuring public trust in the mail.”“The U.S. Department of Veterans Affairs, Office of Inspector General will continue to aggressively investigate those that attempt to fraudulently impact programs designed to benefit our veterans and their families,” said Donna L. Neves, Special Agent in Charge of the VA OIG Northeast Field Office.The charges of conspiracy to commit RICO and conspiracy to commit mail and wire fraud each provide for a sentence of no greater than 20 years in prison, three years of supervised release and a fine of $250,000, or twice the amount of pecuniary gain or loss.\\xa0 The charges of conspiracy to violate the Anti-Kickback Law provide for a sentence of no greater than five years in prison, three years of supervised release and a $25,000 fine. Sentences are imposed by a federal district court judge based upon the U.S. Sentencing Guidelines and other statutory factors.The investigation was conducted by a team that included the FBI; HHS-OIG; FDA Office of Criminal Investigations; the Defense Criminal Investigative Service; the Drug Enforcement Administration; the Department of Labor, Employee Benefits Security Administration; the Office of Personnel Management; the U.S. Postal Inspection Service; the U.S. Postal Service Office of Inspector General; and the Department of Veterans Affairs.\\xa0 The U.S. Attorney’s Office would like to acknowledge the cooperation and assistance of the U.S. Attorney’s Offices around the country engaged in parallel investigations, including the District of Connecticut, Eastern District of Michigan, Southern District of Alabama, Southern District of New York, District of Rhode Island, and the District of New Hampshire.\\xa0 The efforts of the Central District of California and the Justice Department’s Civil Fraud Section of the Department of Justice are also greatly appreciated.\\xa0Assistant U.S. Attorneys K. Nathaniel Yeager, Chief of Weinreb’s Health Care Fraud Unit, and Susan M. Poswistilo, of Weinreb’s Civil Division, are prosecuting the case.The details contained in the charging documents are allegations.\\xa0 The defendants are presumed innocent unless and until proven guilty beyond a reasonable doubt.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"More than 20,000 Americans died of synthetic opioid overdoses last year, and millions are addicted to opioids',\n",
       " 'he charges of conspiracy to commit RICO and conspiracy to commit mail and wire fraud each provide for a sentence of no greater than 20 years in prison, three years of supervised release and a fine of $250,000, or twice the amount of pecuniary gain or loss',\n",
       " ' The charges of conspiracy to violate the Anti-Kickback Law provide for a sentence of no greater than five years in prison, three years of supervised release and a $25,000 fine']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## D\n",
    "# Parse the pharma string at the sentence level.\n",
    "\n",
    "# create a pattern that looks for periods, except those:\n",
    "# 1) preceded by a single capital letter, or\n",
    "# 2) part of \"U.S.\", or\n",
    "# 3) followed by a \",\" \n",
    "sent_pattern = r\"(?<!\\s[A-Z])(?<!U\\.S)\\.[^,]\"    #https://www.regular-expressions.info/lookaround.html\n",
    "\n",
    "# split the string based on this pattern\n",
    "sent_split = re.split(sent_pattern, pharma) \n",
    "#sent_split\n",
    "\n",
    "# Pull and print the original sentences from the press releases where those year lengths are mentioned. \n",
    "year_tokens_pattern = r\"last year|three years\"      # create pattern from the unique year tokens we found before\n",
    "year_quotes = [sentence for sentence in sent_split if re.search(year_tokens_pattern, sentence) is not None]  \n",
    "year_quotes   # print matches\n",
    "\n",
    "# Describe in your own words (1 sentence) what length of sentence (prison) and probation (supervised release) \n",
    "# the CEO may be facing if convicted after this indictment \n",
    "# (if there are multiple lengths mentioned describe the maximum).\n",
    "\n",
    "# Response: The CEO may face up to five years in prison and three years of propation for the Anti-Kickback Law \n",
    "# as well as up to a total of 20 years in prison and three years propoation for RICO and \n",
    "# the same for conspiracy to commit mail and wire fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 sentiment analysis  (10 points)\n",
    "\n",
    "- Sentiment analysis section of this script: \n",
    "https://github.com/rebeccajohnson88/PPOL564_slides_activities/blob/main/activities/fall_22/solutions/08_textasdata_partI_textmining_solutions.ipynb\n",
    "\n",
    "\n",
    "A. Subset the press releases to those labeled with one of three topics via `topics_clean`: Civil Rights, Hate Crimes, and Project Safe Childhood. We'll call this `doj_subset` going forward and it should have 717 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(717, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A\n",
    "\n",
    "# Subset the press releases to those labeled with one of three topics via topics_clean: \n",
    "# Civil Rights, Hate Crimes, and Project Safe Childhood. \n",
    "doj_subset = doj.loc[doj.topics_clean.isin([\"Civil Rights\", \"Hate Crimes\", \"Project Safe Childhood\"])]\n",
    "\n",
    "# Check the shape\n",
    "doj_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Write a function that takes one press release string as an input and:\n",
    "\n",
    "- Removes named entities from each press release string (**Hint:** you may want to use `re.sub` with an or condition)\n",
    "- Scores the sentiment of the entire press release using the `SentimentIntensityAnalyzer` and `polarity_scores`\n",
    "- Returns the length-four (negative, positive, neutral, compound) sentiment dictionary (any order is fine)\n",
    "\n",
    "Apply that function to each of the press releases in `doj_subset`. \n",
    "\n",
    "**Hints**: \n",
    "\n",
    "- I used a function + list comprehension to execute and it takes about 30 seconds on my local machine; if it's taking a very long time, you may want to check your code for inefficiencies. If you can't fix those, for partial credit on this part/full credit on remainder, you can take a small random sample of the 717\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B\n",
    "# Part 1: define a new function\n",
    "\n",
    "def polarity_no_ents(one_string): \n",
    "    '''\n",
    "    Function to return the polarity_scores/sentiment dictionary of a string after removing named entities\n",
    "    \n",
    "    Parameters:\n",
    "        str: press release\n",
    "        \n",
    "    Returns: \n",
    "        pd.DataFrame: polarity_scores/sentiment dictionary \n",
    "    '''\n",
    "    spacy_press = nlp(one_string)      # find named entities \n",
    "    press_ent = [ent.text for ent in spacy_press.ents]     #take the string for each entity\n",
    "    result = re.sub(\"|\".join([re.escape(ent.text) for ent in spacy_press.ents]),\n",
    "                    \" \", one_string)     # re.sub to replace any ents\n",
    "    sent_obj = SentimentIntensityAnalyzer()     # name the analyzer\n",
    "    return sent_obj.polarity_scores(result)     # return the polarity scoes\n",
    "\n",
    "#https://predictivehacks.com/how-to-run-sentiment-analysis-in-python-using-vader/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B Part 2: execute this function\n",
    "sentiment_scores_results = doj_subset.contents.apply(polarity_no_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Add the four sentiment scores to the `doj_subset` dataframe to create a dataframe: `doj_subset_wscore`. Sort from highest neg to lowest neg score (so most negative to least negative) and print the `id`, `contents`, and `neg` columns of the two most negative press releases. \n",
    "\n",
    "Notes:\n",
    "\n",
    "- Don't worry if your sentiment score differs slightly from our output on GitHub; differences in preprocessing can lead to diff scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>contents</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>14-248</td>\n",
       "      <td>The Department of Justice announced that this morning John W. Ng, 58, of Albuquerque, N.M., made his initial appearance in federal court on a criminal complaint charging him with a hate crime offense.  This charge is related to anti-Semitic threats Ng made against a Jewish woman who owns and operates the Nosh Jewish Delicatessen and Bakery in Albuquerque. Ng was arrested by the FBI on March 7, 2014, based on a criminal complaint alleging that he interfered with the victim’s federally protected rights by threatening her and interfering with her business because of her religion.  According to the criminal complaint, between Jan. 22, 2014, and Feb. 8, 2014, Ng allegedly posted threatening anti-Semitic notes on and in the vicinity of the victim’s business. A criminal complaint merely establishes probable cause, and Ng is presumed innocent unless proven guilty.  If convicted on the offense charged in the criminal complaint, Ng faces a maximum statutory penalty of one year in prison. This matter was investigated by the Albuquerque Division of the FBI and is being prosecuted by Assistant U.S. Attorney Mark T. Baker of the U.S. Attorney’s Office for the District of New Mexico and Trial Attorney AeJean Cha of the U.S. Department of Justice’s Civil Rights Division.</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>13-312</td>\n",
       "      <td>John Hall, 27, an Aryan Brotherhood member and inmate at the Federal Correctional Institution (FCI) in Seagoville, Texas, was sentenced today by U.S. District Judge Reed O’Connor after pleading guilty to violating the Matthew Shepard and James Byrd Jr. Hate Crimes Prevention Act stemming from his assault of a fellow inmate, whom he believed to be gay, the Department of Justice announced. Hall assaulted his fellow inmate with a dangerous weapon, causing bodily injury to the victim on Dec. 20, 2011. Hall was sentenced to serve 71 months in prison to be served consecutively with the sentence he is currently serving. The assault occurred on Dec. 20, 2011, inside the FCI Seagoville when Hall targeted and attacked the victim, a fellow inmate, because he believed the victim was gay or involved in a sexual relationship with another male inmate. Hall repeatedly punched, kicked and stomped on the victim’s face with his shod feet, a dangerous weapon, while yelling a homophobic slur. The victim lost consciousness during the assault and suffered multiple lacerations to his face. The victim also sustained a fractured eye socket, lost a tooth, fractured other teeth and was treated at a hospital for the injuries he sustained during Hall’s unprovoked attack. Hall pleaded guilty to violating the Matthew Shepard and James Byrd Jr. Hate Crimes Prevention Act on Nov. 8, 2012. “Brutality and violence based on sexual orientation has no place in a civilized society,” said Thomas E. Perez, Assistant Attorney General for the Civil Rights Division. “The Justice Department is committed to using all the tools in our law enforcement arsenal, including the Matthew Shepard and James Byrd Jr. Hate Crimes Prevention Act, to prosecute acts motivated by hate.” “This prosecution sends a clear message that this office, in partnership with attorneys in the department’s Civil Rights Division, will prioritize and aggressively prosecute hate crimes and others civil rights violations in North Texas,” said U.S. Attorney Sarah R. Saldaña of the Northern District of Texas. This case was investigated by the FBI Dallas Division. The case was prosecuted by Assistant U.S. Attorney Errin Martin and Trial Attorney Adriana Vieco of the Civil Rights Division.</td>\n",
       "      <td>0.301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "329  14-248   \n",
       "572  13-312   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  contents  \\\n",
       "329                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The Department of Justice announced that this morning John W. Ng, 58, of Albuquerque, N.M., made his initial appearance in federal court on a criminal complaint charging him with a hate crime offense.  This charge is related to anti-Semitic threats Ng made against a Jewish woman who owns and operates the Nosh Jewish Delicatessen and Bakery in Albuquerque. Ng was arrested by the FBI on March 7, 2014, based on a criminal complaint alleging that he interfered with the victim’s federally protected rights by threatening her and interfering with her business because of her religion.  According to the criminal complaint, between Jan. 22, 2014, and Feb. 8, 2014, Ng allegedly posted threatening anti-Semitic notes on and in the vicinity of the victim’s business. A criminal complaint merely establishes probable cause, and Ng is presumed innocent unless proven guilty.  If convicted on the offense charged in the criminal complaint, Ng faces a maximum statutory penalty of one year in prison. This matter was investigated by the Albuquerque Division of the FBI and is being prosecuted by Assistant U.S. Attorney Mark T. Baker of the U.S. Attorney’s Office for the District of New Mexico and Trial Attorney AeJean Cha of the U.S. Department of Justice’s Civil Rights Division.   \n",
       "572  John Hall, 27, an Aryan Brotherhood member and inmate at the Federal Correctional Institution (FCI) in Seagoville, Texas, was sentenced today by U.S. District Judge Reed O’Connor after pleading guilty to violating the Matthew Shepard and James Byrd Jr. Hate Crimes Prevention Act stemming from his assault of a fellow inmate, whom he believed to be gay, the Department of Justice announced. Hall assaulted his fellow inmate with a dangerous weapon, causing bodily injury to the victim on Dec. 20, 2011. Hall was sentenced to serve 71 months in prison to be served consecutively with the sentence he is currently serving. The assault occurred on Dec. 20, 2011, inside the FCI Seagoville when Hall targeted and attacked the victim, a fellow inmate, because he believed the victim was gay or involved in a sexual relationship with another male inmate. Hall repeatedly punched, kicked and stomped on the victim’s face with his shod feet, a dangerous weapon, while yelling a homophobic slur. The victim lost consciousness during the assault and suffered multiple lacerations to his face. The victim also sustained a fractured eye socket, lost a tooth, fractured other teeth and was treated at a hospital for the injuries he sustained during Hall’s unprovoked attack. Hall pleaded guilty to violating the Matthew Shepard and James Byrd Jr. Hate Crimes Prevention Act on Nov. 8, 2012. “Brutality and violence based on sexual orientation has no place in a civilized society,” said Thomas E. Perez, Assistant Attorney General for the Civil Rights Division. “The Justice Department is committed to using all the tools in our law enforcement arsenal, including the Matthew Shepard and James Byrd Jr. Hate Crimes Prevention Act, to prosecute acts motivated by hate.” “This prosecution sends a clear message that this office, in partnership with attorneys in the department’s Civil Rights Division, will prioritize and aggressively prosecute hate crimes and others civil rights violations in North Texas,” said U.S. Attorney Sarah R. Saldaña of the Northern District of Texas. This case was investigated by the FBI Dallas Division. The case was prosecuted by Assistant U.S. Attorney Errin Martin and Trial Attorney Adriana Vieco of the Civil Rights Division.   \n",
       "\n",
       "       neg  \n",
       "329   0.33  \n",
       "572  0.301  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## C\n",
    "\n",
    "# Add the four sentiment scores to the doj_subset dataframe to create a dataframe: doj_subset_wscore.\n",
    "\n",
    "# convert each dict to a string \n",
    "doj_subset['sentiment_scores'] = [str(dict) for dict in sentiment_scores_results]\n",
    "\n",
    "# split the sentiment_scores column to four new columns \n",
    "doj_subset[['neg', 'neu', 'pos', 'compound']] = doj_subset.sentiment_scores.str.split(\",\", expand = True) \n",
    "\n",
    "# rewrite the columns with regex to keep only the numbers\n",
    "pattern_int = r\"\\d*(-?[0-9]\\.\\d+)\"\n",
    "# https://stackoverflow.com/questions/15814592/how-do-i-include-negative-decimal-numbers-in-this-regular-expression\n",
    "doj_subset['neg'] = doj_subset.neg.str.extract(pattern_int) \n",
    "doj_subset['neu'] = doj_subset.neu.str.extract(pattern_int) \n",
    "doj_subset['pos'] = doj_subset.pos.str.extract(pattern_int) \n",
    "doj_subset['compound'] = doj_subset.compound.str.extract(pattern_int) \n",
    "\n",
    "# check to see this worked\n",
    "#doj_subset[['id', 'title', 'sentiment_scores', 'neg', 'neu','pos', 'compound']].head()\n",
    "\n",
    "# name as new df\n",
    "doj_subset_wscore = doj_subset.drop(['sentiment_scores'], axis = 1).copy()\n",
    "\n",
    "# check this worked\n",
    "#doj_subset_wscore.head()\n",
    "#type(doj_subset_wscore.neg)\n",
    "\n",
    "# Sort from highest neg to lowest neg score (so most negative to least negative) \n",
    "neg_sort_desc = doj_subset_wscore.sort_values(by = ['neg'], ascending = False).copy()\n",
    "\n",
    "# Print the id, contents, and neg columns of the two most negative press releases.\n",
    "neg_sort_desc[['id', 'contents', 'neg']].iloc[0:2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. With the dataframe from part C, find the mean compound sentiment score for each of the three topics in `topics_clean` using group_by and agg.\n",
    "\n",
    "E. Add a 1 sentence interpretation of why we might see the variation in scores (remember that compound is a standardized summary where -1 is most negative; +1 is most positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 717 entries, 77 to 13081\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   id                707 non-null    object \n",
      " 1   title             717 non-null    object \n",
      " 2   contents          717 non-null    object \n",
      " 3   date              717 non-null    object \n",
      " 4   topics_clean      717 non-null    object \n",
      " 5   components_clean  717 non-null    object \n",
      " 6   neg               717 non-null    float64\n",
      " 7   neu               717 non-null    float64\n",
      " 8   pos               717 non-null    float64\n",
      " 9   compound          717 non-null    float64\n",
      "dtypes: float64(4), object(6)\n",
      "memory usage: 61.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topics_clean</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Civil Rights</th>\n",
       "      <td>-0.093398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hate Crimes</th>\n",
       "      <td>-0.935567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Project Safe Childhood</th>\n",
       "      <td>-0.664434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        compound\n",
       "topics_clean                    \n",
       "Civil Rights           -0.093398\n",
       "Hate Crimes            -0.935567\n",
       "Project Safe Childhood -0.664434"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#D\n",
    "# agg and find the mean compound score by topic\n",
    "\n",
    "# first need to convert scored columns to numeric (float) values\n",
    "doj_subset_wscore['neg'] = doj_subset_wscore['neg'].astype(float) \n",
    "doj_subset_wscore['neu'] = doj_subset_wscore['neu'].astype(float) \n",
    "doj_subset_wscore['pos'] = doj_subset_wscore['pos'].astype(float) \n",
    "doj_subset_wscore['compound'] = doj_subset_wscore['compound'].astype(float) \n",
    "\n",
    "# check this worked\n",
    "doj_subset_wscore.info()\n",
    "\n",
    "# then agg and find the mean compound score by topic\n",
    "mean_compound = doj_subset_wscore.groupby(\"topics_clean\").agg({\"compound\": \"mean\"}).copy()\n",
    "mean_compound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E. RESPONSE:** As we can see, hate crimes have the strongest negative mean since the value is closest to 1, although all three topics are closer to the the most negative value (-1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic modeling (25 points)\n",
    "\n",
    "For this question, use the `doj_subset_wscores` data that is restricted to civil rights, hate crimes, and project safe childhood and with the sentiment scores added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocess the data by removing stopwords, punctuation, and non-alpha words (5 points)\n",
    "\n",
    "A. Write a function that:\n",
    "\n",
    "- Takes in a single raw string in the `contents` column from that dataframe\n",
    "- Does the following preprocessing steps:\n",
    "\n",
    "    - Converts the words to lowercase\n",
    "    - Removes stopwords, adding the custom stopwords in the code cell below to the default stopwords list\n",
    "    - Only retains alpha words (so removes digits and punctuation)\n",
    "    - Only retains words 4 characters or longer\n",
    "    - Uses the snowball stemmer from nltk to stem\n",
    "\n",
    "- Returns a joined preprocessed string (so if press release is something like \"The CEO was indicted\" it might return \"ceo indict\" \n",
    "    \n",
    "B. Use `apply` or list comprehension to execute that function and create a new column in the data called `processed_text`. Note: there will be a deduction if your code uses a non-list comprehension for loop that uses append.\n",
    "    \n",
    "C. Print the `id`, `contents`, and `processed_text` columns for the following press releases:\n",
    "\n",
    "id = 16-718 (this case: https://www.seattletimes.com/nation-world/doj-miami-police-reach-settlement-in-civil-rights-case/)\n",
    "\n",
    "id = 16-217 (this case: https://www.wlbt.com/story/32275512/three-mississippi-correctional-officers-indicted-for-inmate-assault-and-cover-up/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_doj_stopwords = [\"civil\", \"rights\", \"division\", \"department\", \"justice\",\n",
    "                        \"office\", \"attorney\", \"district\", \"case\", \"investigation\", \"assistant\",\n",
    "                       \"trial\", \"assistance\", \"assist\"]\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words_new = stop_words + custom_doj_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Define a text processing function\n",
    "\n",
    "def text_process(one_string):\n",
    "    '''Takes in a single raw string and does the following preprocessing steps: \n",
    "    1) Converts the words to lowercase\n",
    "    2) Removes stopwords, adding the custom stopwords in the code cell below to the default stopwords list' \n",
    "    3) Only retains alpha words (so removes digits and punctuation)\n",
    "    4) Only retains words 4 characters or longer\n",
    "    5) Uses the snowball stemmer from nltk to stem\n",
    "    \n",
    "    Parameters:\n",
    "        str\n",
    "    \n",
    "    Returns:\n",
    "        a joined preprocessed string'''\n",
    "    \n",
    "    one_string.lower() # Converts the words to lowercase\n",
    "    wordpunct_tokenize(one_string) # tokenize\n",
    "    no_stop_string = [word for word in wordpunct_tokenize(one_string) # Removes stopwords (custome list above)\n",
    "                      if word not in stop_words_new]\n",
    "    snowball = SnowballStemmer(language = 'english')  # Uses the snowball stemmer from nltk to stem\n",
    "    preprocessed = [snowball.stem(token) for token in no_stop_string if \n",
    "                    token.isalpha() and   # Only retains alpha words (so removes digits and punctuation)\n",
    "                    len(token) > 3]  # Only retains words 4 characters or longer\n",
    "    clean_string = \" \".join(preprocessed)\n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B\n",
    "#  Use apply or list comprehension to execute that function and create a new column in the data called \n",
    "#  processed_text. \n",
    "#  Note: there will be a deduction if your code uses a non-list comprehension for loop that uses append.\n",
    "\n",
    "doj_subset_wscore['processed_text'] = [text_process(string) for string in doj_subset_wscore.contents]\n",
    "\n",
    "# test to see this worked\n",
    "#doj_subset_wscore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>contents</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6727</th>\n",
       "      <td>16-217</td>\n",
       "      <td>The Justice Department has reached a comprehensive settlement agreement with the city of Miami and the Miami Police Department (MPD) resolving the Justice Department’s investigation of officer-involved shootings by MPD officers, announced Principal Deputy Assistant Attorney General Vanita Gupta, head of the Justice Department’s Civil Rights Division and U.S. Attorney Wifredo A. Ferrer of the Southern District of Florida. The settlement, which was approved by Miami’s city commission today and will go into effect when the agreement is signed by all parties, resolves claims stemming from the Justice Department’s investigation into officer-involved shootings by MPD officers, which was conducted under the Violent Crime Control and Law Enforcement Act of 1994.  The investigation’s findings, issued in July 2013, identified a pattern or practice of excessive use of force through officer-involved shootings in violation of the Fourth Amendment of the Constitution.  The city’s compliance with the settlement will be monitored by an independent reviewer, former Tampa, Florida, Police Chief Jane Castor.  Under the settlement agreement, the city will implement comprehensive reforms to ensure constitutional policing and support public trust.  The settlement agreement is designed to minimize officer-involved shootings and to more effectively and quickly investigate officer-involved shootings that do occur, through measures that include: “This settlement represents a renewed commitment by the city of Miami and Chief Rodolfo Llanes to provide constitutional policing for Miami residents and to protect public safety through sustainable reform,” said Principal Deputy Assistant Attorney General Gupta.  “The agreement will help to strengthen the relationship between the MPD and the communities they serve by improving accountability for officers who fire their weapons unlawfully, and provides for community participation in the enforcement of this agreement.”  “Today's agreement is the result of a joint effort between the Department of Justice and the City of Miami to ensure that the Miami Police Department continues its efforts to make our community safe while protecting the sacred Constitutional rights of all of our citizens,” said U.S. Attorney Ferrer.  “Through oversight and communication, the agreement seeks to make permanent the positive changes that former Chief Orosa and Chief Llanes have made, and we applaud the City Commission’s vote.” The settlement agreement builds upon important reforms implemented by the city since the Justice Department issued its findings, including:  The investigation was conducted by attorneys and staff from the Civil Rights Division’s Special Litigation Section and the Civil Division of the U. S. Attorney’s Office of the Southern District of Florida.</td>\n",
       "      <td>justic depart reach comprehens settlement agreement citi miami miami polic depart resolv justic depart offic involv shoot offic announc princip deputi assist attorney general vanita gupta head justic depart civil right divis attorney wifredo ferrer southern district florida settlement approv miami citi commiss today effect agreement sign parti resolv claim stem justic depart offic involv shoot offic conduct violent crime control enforc find issu juli identifi pattern practic excess forc offic involv shoot violat fourth amend constitut citi complianc settlement monitor independ review former tampa florida polic chief jane castor under settlement agreement citi implement comprehens reform ensur constitut polic support public trust settlement agreement design minim offic involv shoot effect quick investig offic involv shoot occur measur includ this settlement repres renew commit citi miami chief rodolfo llane provid constitut polic miami resid protect public safeti sustain reform said princip deputi assist attorney general gupta agreement help strengthen relationship communiti serv improv account offic fire weapon unlaw provid communiti particip enforc agreement today agreement result joint effort depart justic citi miami ensur miami polic depart continu effort make communiti safe protect sacr constitut citizen said attorney ferrer through oversight communic agreement seek make perman posit chang former chief orosa chief llane made applaud citi commiss vote settlement agreement build upon import reform implement citi sinc justic depart issu find includ conduct attorney staff civil right divis special litig section civil divis attorney offic southern district florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11593</th>\n",
       "      <td>16-718</td>\n",
       "      <td>In a nine-count indictment unsealed today, two Mississippi correctional officers were charged with beating an inmate and a third was charged with helping to cover it up.  The indictment charged Lawardrick Marsher, 28, and Robert Sturdivant, 47, officers at Mississippi State Penitentiary, in Parchman, Mississippi, with a beating that included kicking, punching and throwing the victim to the ground.  Marsher and Sturdivant were charged with violating the right of K.H., a convicted prisoner, to be free from cruel and unusual punishment.  Sturdivant was also charged with failing to intervene while Marsher was punching and beating K.H.  The indictment alleges that their actions involved the use of a dangerous weapon and resulted in bodily injury to the victim. A third officer, Deonte Pate, 23, was charged along with Marsher and Sturdivant for conspiring to cover up the beating.  The indictment alleges that all three officers submitted false reports and that all three lied to the FBI. If convicted, Marsher and Sturdivant face a maximum sentence of 10 years in prison on the excessive force charges.  Each of the three officers faces up to five years in prison on the conspiracy and false statement charges, and up to 20 years in prison on the false report charges. An indictment is merely an accusation, and the defendants are presumed innocent unless and until proven guilty. This case is being investigated by the FBI’s Jackson Division, with the cooperation of the Mississippi Department of Corrections.  It is being prosecuted by Assistant U.S. Attorney Robert Coleman of the Northern District of Mississippi and Trial Attorney Dana Mulhauser of the Civil Rights Division’s Criminal Section.     Marsher Indictment</td>\n",
       "      <td>nine count indict unseal today mississippi correct offic charg beat inmat third charg help cover indict charg lawardrick marsher robert sturdiv offic mississippi state penitentiari parchman mississippi beat includ kick punch throw victim ground marsher sturdiv charg violat right convict prison free cruel unusu punish sturdiv also charg fail interven marsher punch beat indict alleg action involv danger weapon result bodili injuri victim third offic deont pate charg along marsher sturdiv conspir cover beat indict alleg three offic submit fals report three lie convict marsher sturdiv face maximum sentenc year prison excess forc charg each three offic face five year prison conspiraci fals statement charg year prison fals report charg indict mere accus defend presum innoc unless proven guilti this investig jackson divis cooper mississippi depart correct prosecut assist attorney robert coleman northern district mississippi trial attorney dana mulhaus civil right divis crimin section marsher indict</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  \\\n",
       "6727   16-217   \n",
       "11593  16-718   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         contents  \\\n",
       "6727   The Justice Department has reached a comprehensive settlement agreement with the city of Miami and the Miami Police Department (MPD) resolving the Justice Department’s investigation of officer-involved shootings by MPD officers, announced Principal Deputy Assistant Attorney General Vanita Gupta, head of the Justice Department’s Civil Rights Division and U.S. Attorney Wifredo A. Ferrer of the Southern District of Florida. The settlement, which was approved by Miami’s city commission today and will go into effect when the agreement is signed by all parties, resolves claims stemming from the Justice Department’s investigation into officer-involved shootings by MPD officers, which was conducted under the Violent Crime Control and Law Enforcement Act of 1994.  The investigation’s findings, issued in July 2013, identified a pattern or practice of excessive use of force through officer-involved shootings in violation of the Fourth Amendment of the Constitution.  The city’s compliance with the settlement will be monitored by an independent reviewer, former Tampa, Florida, Police Chief Jane Castor.  Under the settlement agreement, the city will implement comprehensive reforms to ensure constitutional policing and support public trust.  The settlement agreement is designed to minimize officer-involved shootings and to more effectively and quickly investigate officer-involved shootings that do occur, through measures that include: “This settlement represents a renewed commitment by the city of Miami and Chief Rodolfo Llanes to provide constitutional policing for Miami residents and to protect public safety through sustainable reform,” said Principal Deputy Assistant Attorney General Gupta.  “The agreement will help to strengthen the relationship between the MPD and the communities they serve by improving accountability for officers who fire their weapons unlawfully, and provides for community participation in the enforcement of this agreement.”  “Today's agreement is the result of a joint effort between the Department of Justice and the City of Miami to ensure that the Miami Police Department continues its efforts to make our community safe while protecting the sacred Constitutional rights of all of our citizens,” said U.S. Attorney Ferrer.  “Through oversight and communication, the agreement seeks to make permanent the positive changes that former Chief Orosa and Chief Llanes have made, and we applaud the City Commission’s vote.” The settlement agreement builds upon important reforms implemented by the city since the Justice Department issued its findings, including:  The investigation was conducted by attorneys and staff from the Civil Rights Division’s Special Litigation Section and the Civil Division of the U. S. Attorney’s Office of the Southern District of Florida.   \n",
       "11593                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            In a nine-count indictment unsealed today, two Mississippi correctional officers were charged with beating an inmate and a third was charged with helping to cover it up.  The indictment charged Lawardrick Marsher, 28, and Robert Sturdivant, 47, officers at Mississippi State Penitentiary, in Parchman, Mississippi, with a beating that included kicking, punching and throwing the victim to the ground.  Marsher and Sturdivant were charged with violating the right of K.H., a convicted prisoner, to be free from cruel and unusual punishment.  Sturdivant was also charged with failing to intervene while Marsher was punching and beating K.H.  The indictment alleges that their actions involved the use of a dangerous weapon and resulted in bodily injury to the victim. A third officer, Deonte Pate, 23, was charged along with Marsher and Sturdivant for conspiring to cover up the beating.  The indictment alleges that all three officers submitted false reports and that all three lied to the FBI. If convicted, Marsher and Sturdivant face a maximum sentence of 10 years in prison on the excessive force charges.  Each of the three officers faces up to five years in prison on the conspiracy and false statement charges, and up to 20 years in prison on the false report charges. An indictment is merely an accusation, and the defendants are presumed innocent unless and until proven guilty. This case is being investigated by the FBI’s Jackson Division, with the cooperation of the Mississippi Department of Corrections.  It is being prosecuted by Assistant U.S. Attorney Robert Coleman of the Northern District of Mississippi and Trial Attorney Dana Mulhauser of the Civil Rights Division’s Criminal Section.     Marsher Indictment   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    processed_text  \n",
       "6727   justic depart reach comprehens settlement agreement citi miami miami polic depart resolv justic depart offic involv shoot offic announc princip deputi assist attorney general vanita gupta head justic depart civil right divis attorney wifredo ferrer southern district florida settlement approv miami citi commiss today effect agreement sign parti resolv claim stem justic depart offic involv shoot offic conduct violent crime control enforc find issu juli identifi pattern practic excess forc offic involv shoot violat fourth amend constitut citi complianc settlement monitor independ review former tampa florida polic chief jane castor under settlement agreement citi implement comprehens reform ensur constitut polic support public trust settlement agreement design minim offic involv shoot effect quick investig offic involv shoot occur measur includ this settlement repres renew commit citi miami chief rodolfo llane provid constitut polic miami resid protect public safeti sustain reform said princip deputi assist attorney general gupta agreement help strengthen relationship communiti serv improv account offic fire weapon unlaw provid communiti particip enforc agreement today agreement result joint effort depart justic citi miami ensur miami polic depart continu effort make communiti safe protect sacr constitut citizen said attorney ferrer through oversight communic agreement seek make perman posit chang former chief orosa chief llane made applaud citi commiss vote settlement agreement build upon import reform implement citi sinc justic depart issu find includ conduct attorney staff civil right divis special litig section civil divis attorney offic southern district florida  \n",
       "11593                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               nine count indict unseal today mississippi correct offic charg beat inmat third charg help cover indict charg lawardrick marsher robert sturdiv offic mississippi state penitentiari parchman mississippi beat includ kick punch throw victim ground marsher sturdiv charg violat right convict prison free cruel unusu punish sturdiv also charg fail interven marsher punch beat indict alleg action involv danger weapon result bodili injuri victim third offic deont pate charg along marsher sturdiv conspir cover beat indict alleg three offic submit fals report three lie convict marsher sturdiv face maximum sentenc year prison excess forc charg each three offic face five year prison conspiraci fals statement charg year prison fals report charg indict mere accus defend presum innoc unless proven guilti this investig jackson divis cooper mississippi depart correct prosecut assist attorney robert coleman northern district mississippi trial attorney dana mulhaus civil right divis crimin section marsher indict  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## C\n",
    "# Print the id, contents, and processed_text columns for the following press releases:\n",
    "example_subset = doj_subset_wscore[['id', 'contents', 'processed_text']].loc[doj_subset_wscore.id.isin([\"16-718\",\"16-217\"])]\n",
    "example_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create a document-term matrix from the preprocessed press releases and to explore top words (5 points)\n",
    "\n",
    "A. Use the `create_dtm` function I provide (alternately, feel free to write your own!) and create a document-term matrix using the preprocessed press releases; make sure metadata contains the following columns: `id`, `compound` sentiment column you added, and the `topics_clean` column\n",
    "\n",
    "B. Print the top 10 words for press releases with compound sentiment in the top 5% (so the most positive sentiment)\n",
    "\n",
    "C. Print the top 10 words for press releases with compound sentiment in the bottom 5% (so the most negative sentiment)\n",
    "\n",
    "**Hint**: for these, remember the pandas quantile function from pset two.  \n",
    "\n",
    "D. Print the top 10 words for press releases in each of the three `topics_clean`\n",
    "\n",
    "For steps B - D, to receive full credit, write a function `get_topwords` that helps you avoid duplicated code when you find top words for the different subsets of the data. There are different ways to structure it but one way is to feed it subsetted data (so data subsetted to one topic etc.) and for it to get the top words for that subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), \n",
    "        columns=vectorizer.get_feature_names())\n",
    "    metadata.columns = [\"metadata_\" + col for col in metadata.columns] #added in column renaming \n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named_withid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>metadata_id</th>\n",
       "      <th>metadata_compound</th>\n",
       "      <th>metadata_topics_clean</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbat</th>\n",
       "      <th>abbi</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>...</th>\n",
       "      <th>zane</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zeeman</th>\n",
       "      <th>zero</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zobel</th>\n",
       "      <th>zone</th>\n",
       "      <th>zunggeemog</th>\n",
       "      <th>zwengel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>17-1235</td>\n",
       "      <td>-0.9931</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>15-1522</td>\n",
       "      <td>-0.9325</td>\n",
       "      <td>Project Safe Childhood</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157</td>\n",
       "      <td>16-213</td>\n",
       "      <td>-0.7579</td>\n",
       "      <td>Project Safe Childhood</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>162</td>\n",
       "      <td>16-381</td>\n",
       "      <td>-0.9037</td>\n",
       "      <td>Project Safe Childhood</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168</td>\n",
       "      <td>14-464</td>\n",
       "      <td>-0.9864</td>\n",
       "      <td>Hate Crimes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index metadata_id  metadata_compound   metadata_topics_clean  aaron  \\\n",
       "0     77     17-1235            -0.9931            Civil Rights      0   \n",
       "1    155     15-1522            -0.9325  Project Safe Childhood      0   \n",
       "2    157      16-213            -0.7579  Project Safe Childhood      0   \n",
       "3    162      16-381            -0.9037  Project Safe Childhood      0   \n",
       "4    168      14-464            -0.9864             Hate Crimes      0   \n",
       "\n",
       "   abandon  abbat  abbi  abbott  abdomen  ...  zane  zealand  zealous  zeeman  \\\n",
       "0        0      0     0       0        0  ...     0        0        0       0   \n",
       "1        0      0     0       0        0  ...     0        0        0       0   \n",
       "2        0      0     0       0        0  ...     0        0        0       0   \n",
       "3        0      0     0       0        0  ...     0        0        0       0   \n",
       "4        0      0     0       0        0  ...     0        0        0       0   \n",
       "\n",
       "   zero  zionism  zobel  zone  zunggeemog  zwengel  \n",
       "0     0        0      0     0           0        0  \n",
       "1     0        0      0     0           0        0  \n",
       "2     0        0      0     0           0        0  \n",
       "3     0        0      0     0           0        0  \n",
       "4     0        0      0     0           0        0  \n",
       "\n",
       "[5 rows x 6905 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A \n",
    "# Use the create_dtm function I provide and create a document-term matrix using the preprocessed press releases; \n",
    "# make sure metadata contains the following columns: id, compound sentiment column you added, \n",
    "   # and the topics_clean column\n",
    "metadata = [\"id\", \"compound\", \"topics_clean\"]\n",
    "\n",
    "doj_doc_matrix = create_dtm(list_of_strings = doj_subset_wscore.processed_text,\n",
    "                        metadata = doj_subset_wscore[metadata])\n",
    "\n",
    "doj_doc_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to find top words of this subset\n",
    "def top_words(df_subset):\n",
    "    \n",
    "    '''Takes a subset dataframe, sums the columns, \n",
    "    and and returns a series of the top 10 sums\n",
    "    \n",
    "    Parameters: subset pd.dataframe\n",
    "    \n",
    "    Returns: series'''\n",
    "    \n",
    "    top_terms = df_subset[[col for col in df_subset.columns\n",
    "                               if \"metadata\" not in col and \n",
    "                               col != \"index\"]].sum(axis = 0)\n",
    "    return top_terms.sort_values(ascending = False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>metadata_id</th>\n",
       "      <th>metadata_compound</th>\n",
       "      <th>metadata_topics_clean</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbat</th>\n",
       "      <th>abbi</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>...</th>\n",
       "      <th>zane</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zeeman</th>\n",
       "      <th>zero</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zobel</th>\n",
       "      <th>zone</th>\n",
       "      <th>zunggeemog</th>\n",
       "      <th>zwengel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>683</td>\n",
       "      <td>11-645</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>Project Safe Childhood</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1857</td>\n",
       "      <td>17-271</td>\n",
       "      <td>0.9909</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2394</td>\n",
       "      <td>16-525</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2402</td>\n",
       "      <td>16-1224</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2996</td>\n",
       "      <td>16-577</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index metadata_id  metadata_compound   metadata_topics_clean  aaron  \\\n",
       "35     683      11-645             0.9974  Project Safe Childhood      0   \n",
       "72    1857      17-271             0.9909            Civil Rights      0   \n",
       "89    2394      16-525             0.9960            Civil Rights      0   \n",
       "90    2402     16-1224             0.9948            Civil Rights      0   \n",
       "101   2996      16-577             0.9933            Civil Rights      0   \n",
       "\n",
       "     abandon  abbat  abbi  abbott  abdomen  ...  zane  zealand  zealous  \\\n",
       "35         0      0     0       0        0  ...     0        0        0   \n",
       "72         0      0     0       0        0  ...     0        0        0   \n",
       "89         0      0     0       0        0  ...     0        0        0   \n",
       "90         0      0     0       0        0  ...     0        0        0   \n",
       "101        0      0     0       0        0  ...     0        0        0   \n",
       "\n",
       "     zeeman  zero  zionism  zobel  zone  zunggeemog  zwengel  \n",
       "35        0     0        0      0     0           0        0  \n",
       "72        0     0        0      0     0           0        0  \n",
       "89        0     0        0      0     0           0        0  \n",
       "90        0     0        0      0     0           0        0  \n",
       "101       0     0        0      0     0           0        0  \n",
       "\n",
       "[5 rows x 6905 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "depart       185\n",
       "agreement    177\n",
       "attorney     139\n",
       "right        132\n",
       "justic       130\n",
       "enforc       121\n",
       "civil        116\n",
       "state        114\n",
       "offic        110\n",
       "disabl       108\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## B\n",
    "# Print the top 10 words for press releases with compound sentiment in the top 5% \n",
    "\n",
    "# first, subset to greater than 95% compound scores\n",
    "most_pos_subset = doj_doc_matrix[doj_doc_matrix.metadata_compound >= doj_doc_matrix.metadata_compound.quantile(0.95)].copy()\n",
    "most_pos_subset.head()\n",
    "\n",
    "# apply function \n",
    "top_words(most_pos_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>metadata_id</th>\n",
       "      <th>metadata_compound</th>\n",
       "      <th>metadata_topics_clean</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbat</th>\n",
       "      <th>abbi</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>...</th>\n",
       "      <th>zane</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zeeman</th>\n",
       "      <th>zero</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zobel</th>\n",
       "      <th>zone</th>\n",
       "      <th>zunggeemog</th>\n",
       "      <th>zwengel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>495</td>\n",
       "      <td>11-648</td>\n",
       "      <td>-0.9985</td>\n",
       "      <td>Hate Crimes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>501</td>\n",
       "      <td>11-626</td>\n",
       "      <td>-0.9986</td>\n",
       "      <td>Hate Crimes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>513</td>\n",
       "      <td>11-1275</td>\n",
       "      <td>-0.9980</td>\n",
       "      <td>Hate Crimes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>572</td>\n",
       "      <td>13-312</td>\n",
       "      <td>-0.9983</td>\n",
       "      <td>Hate Crimes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1357</td>\n",
       "      <td>14-309</td>\n",
       "      <td>-0.9982</td>\n",
       "      <td>Hate Crimes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index metadata_id  metadata_compound metadata_topics_clean  aaron  \\\n",
       "17    495      11-648            -0.9985           Hate Crimes      0   \n",
       "22    501      11-626            -0.9986           Hate Crimes      0   \n",
       "29    513     11-1275            -0.9980           Hate Crimes      0   \n",
       "34    572      13-312            -0.9983           Hate Crimes      0   \n",
       "58   1357      14-309            -0.9982           Hate Crimes      0   \n",
       "\n",
       "    abandon  abbat  abbi  abbott  abdomen  ...  zane  zealand  zealous  \\\n",
       "17        0      0     0       0        0  ...     0        0        0   \n",
       "22        0      0     0       0        0  ...     0        0        0   \n",
       "29        0      0     0       0        0  ...     0        0        0   \n",
       "34        0      0     0       0        0  ...     0        0        0   \n",
       "58        0      0     0       0        1  ...     0        0        0   \n",
       "\n",
       "    zeeman  zero  zionism  zobel  zone  zunggeemog  zwengel  \n",
       "17       0     0        0      0     0           0        0  \n",
       "22       0     0        0      0     0           0        0  \n",
       "29       0     0        0      0     0           0        0  \n",
       "34       0     0        0      0     0           0        0  \n",
       "58       0     0        0      0     0           0        0  \n",
       "\n",
       "[5 rows x 6905 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "offic       245\n",
       "attorney    213\n",
       "assault     192\n",
       "crime       179\n",
       "victim      166\n",
       "depart      156\n",
       "right       152\n",
       "divis       138\n",
       "hate        134\n",
       "defend      128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## C\n",
    "# Print the top 10 words for press releases with compound sentiment in the bottom 5% \n",
    "\n",
    "# first, subset to less than 5% compound scores\n",
    "most_neg_subset = doj_doc_matrix[doj_doc_matrix.metadata_compound <= doj_doc_matrix.metadata_compound.quantile(0.05)].copy()\n",
    "most_neg_subset.head()\n",
    "\n",
    "# apply function \n",
    "top_words(most_neg_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attorney    1086\n",
       "right        656\n",
       "divis        645\n",
       "depart       628\n",
       "victim       591\n",
       "district     562\n",
       "crime        557\n",
       "civil        548\n",
       "hate         524\n",
       "defend       484\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "depart       1122\n",
       "right         949\n",
       "attorney      942\n",
       "offic         899\n",
       "justic        894\n",
       "civil         793\n",
       "divis         772\n",
       "district      670\n",
       "hous          633\n",
       "discrimin     616\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "child          1022\n",
       "attorney        878\n",
       "exploit         701\n",
       "district        589\n",
       "sexual          572\n",
       "safe            479\n",
       "childhood       474\n",
       "project         472\n",
       "pornographi     452\n",
       "depart          442\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## D\n",
    "\n",
    "# Print the top 10 words for press releases in each of the three topics_clean\n",
    "hate_crimes_subset = doj_doc_matrix.loc[doj_doc_matrix.metadata_topics_clean == \"Hate Crimes\"]\n",
    "top_words(hate_crimes_subset)\n",
    "\n",
    "civil_rights_subset = doj_doc_matrix.loc[doj_doc_matrix.metadata_topics_clean == \"Civil Rights\"]\n",
    "top_words(civil_rights_subset)\n",
    "\n",
    "safe_child_subset = doj_doc_matrix.loc[doj_doc_matrix.metadata_topics_clean == \"Project Safe Childhood\"]\n",
    "top_words(safe_child_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Estimate a topic model using those preprocessed words (5 points)\n",
    "\n",
    "A. Going back to the preprocessed words from part 2.1, estimate a topic model with 3 topics, since you want to see if the unsupervised topic models recover different themes for each of the three manually-labeled topics (civil rights; hate crimes; project safe childhood). You have free rein over the other topic model parameters beyond the number of topics.\n",
    "\n",
    "B. After estimating the topic model, print the top 15 words in each topic.\n",
    "\n",
    "**Hints and Resources**:\n",
    "\n",
    "- Same topic modeling resources linked to above\n",
    "- Make sure to use the `random_state` argument within the model so that the numbering of topics does not move around between runs of your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A\n",
    "# estimate a topic model with 3 topics, since you want to see if the unsupervised topic models recover different \n",
    "# themes for each of the three manually-labeled topics (civil rights; hate crimes; project safe childhood). \n",
    "\n",
    "# Step 1: tokenize documents and store in list\n",
    "processed_tokens = [wordpunct_tokenize(one_text) for one_text in doj_subset_wscore.processed_text]\n",
    "\n",
    "# QUESTION: didn't we already do this? ANS: my function before joined them together again, so now we need to repeat\n",
    "\n",
    "# Step 2: Use gensim create dictionary to get all unique words across the documents\n",
    "text_raw_dict = corpora.Dictionary(processed_tokens)\n",
    "\n",
    "# Step 3: filter out the very rare and very common words from the dict; feeding it n docs as lower and upper bounds\n",
    "lower_bound = round(doj_subset_wscore.shape[0]*0.05)\n",
    "upper_bound = round(doj_subset_wscore.shape[0]*0.95)\n",
    "\n",
    "#lower_bound\n",
    "#upper_bound\n",
    "\n",
    "# QUESTION: what is this doing? ANSWER: this is just finding the number of rows where the cutoff is for 5 percent etc. \n",
    "\n",
    "text_raw_dict.filter_extremes(no_below = lower_bound,\n",
    "                              no_above = upper_bound)\n",
    "\n",
    "# Step 4: apply dictionary to TOKENIZED texts\n",
    "corpus_fromdict = [text_raw_dict.doc2bow(one_text) \n",
    "                   for one_text in processed_tokens]\n",
    "\n",
    "# Step 5: estimate a model by feeding it:\n",
    "# 1) the corpus mapped to the dictionary\n",
    "# 2) the dictionary itself \n",
    "# 3) the number of topics (often k if you're reading notation)\n",
    "# 4) +  any other assorted other arguments\n",
    "\n",
    "ldamod = gensim.models.ldamodel.LdaModel(corpus_fromdict, \n",
    "                                num_topics = 3, id2word=text_raw_dict, \n",
    "                                passes=50, alpha = 'auto',\n",
    "                                per_word_topics = True, random_state = 91988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B\n",
    "# After estimating the topic model, print the top 15 words in each topic.\n",
    "topics = ldamod.print_topics(num_words = 15)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Add topics back to main data and explore correlation between manual labels and our estimated topics (10 points)\n",
    "\n",
    "A. Extract the document-level topic probabilities. Within `get_document_topics`, use the argument `minimum_probability` = 0 to make sure all 3 topic probabilities are returned. Write an assert statement to make sure the length of the list is equal to the number of rows in the `doj_subset_wscores` dataframe\n",
    "\n",
    "B. Add the topic probabilities to the `doj_subset_wscores` dataframe as columns and create a column, `top_topic`, that reflects each document to its highest-probability topic (eg topic 1, 2, or 3)\n",
    "\n",
    "C. For each of the manual labels in `topics_clean` (Hate Crime, Civil Rights, Project Safe Childhood), print the breakdown of the % of documents with each top topic (so, for instance, Hate Crime has 246 documents-- if 123 of those documents are coded to topic_1, that would be 50%; and so on). \n",
    "**Hint**:    \n",
    "pd.crosstab and normalize may be helpful: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.crosstab.html\n",
    "\n",
    "D. Using a couple press releases as examples, write a 1-2 sentence interpretation of why some of the manual topics map on more cleanly to an estimated topic than other manual topic(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A \n",
    "\n",
    "# Extract the document-level topic probabilities\n",
    "topic_probs_bydoc = [ldamod.get_document_topics(item, minimum_probability = 0) for item in corpus_fromdict]\n",
    "\n",
    "# Write an assert statement to make sure the length of the list is the same # of rows as our df\n",
    "doj_subset_wscore.shape\n",
    "assert len(topic_probs_bydoc) == 717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B \n",
    "\n",
    "# Add the topic probabilities to the doj_subset_wscores dataframe as columns and create a column (top_topic)\n",
    "# that reflects each document to its highest-probability topic (eg topic 1, 2, or 3)\n",
    "\n",
    "# check topic probs for one example\n",
    "one_list_tup = topic_probs_bydoc[0]\n",
    "one_list_tup\n",
    "\n",
    "# step 1: create a long for dataframe by flattening the list\n",
    "topic_probs_bydoc_long = pd.DataFrame([t for lst in topic_probs_bydoc for t in lst],\n",
    "                                     columns = ['topic', 'probability'])\n",
    "\n",
    "# step 2: add the id var - we're repeating each id in the original data k times for the number of topics \n",
    "topic_probs_bydoc_long['doc_id'] = list(np.concatenate([[one_id] * 3 \n",
    "                                                      for one_id in doj_subset_wscore.id]).flat)\n",
    "\n",
    "# step 3: pivot to wide format\n",
    "topic_probs_bydoc_wide = pd.pivot_table(topic_probs_bydoc_long, index = ['doc_id'],\n",
    "                                       columns = ['topic']).reset_index().reset_index(drop = True)\n",
    "\n",
    "topic_probs_bydoc_wide.columns = ['doc_id'] + [\"topic_\" + str(i) for i in np.arange(0, 3)]\n",
    "\n",
    "# step 4: merge with original data using doc id\n",
    "topic_wmeta = pd.merge(topic_probs_bydoc_wide, \n",
    "                       doj_subset_wscore,\n",
    "                       left_on = 'doc_id',\n",
    "                       right_on = 'id')\n",
    "\n",
    "# step 5: create indicator for listing's top topic\n",
    "topic_wmeta['top_topic'] = topic_wmeta[[col for col in topic_wmeta.columns if \n",
    "                                       \"topic_\" in col]].idxmax(axis = 1)\n",
    "\n",
    "# check to see this worked \n",
    "#topic_wmeta.sample(5, random_state = 555)\n",
    "topic_wmeta_small = topic_wmeta.drop([\"contents\", \"date\", \"processed_text\"], axis = 1).copy()\n",
    "topic_wmeta_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## C \n",
    "# your code here to summarize the topic proportions for each of the topics_clean \n",
    "#  For each of the manual labels in topics_clean (Hate Crime, Civil Rights, Project Safe Childhood), \n",
    "#print the breakdown of the % of documents with each top topic \n",
    "# (so, for instance, Hate Crime has 246 documents-- if 123 of those documents are coded to topic_1, \n",
    "#that would be 50%; and so on).\n",
    "\n",
    "# First try\n",
    "topic_wmeta_small.groupby(\"topics_clean\").agg({\"topic_0\": np.mean,\n",
    "                                              \"topic_1\": np.mean, \n",
    "                                              \"topic_2\": np.mean})\n",
    "\n",
    "# Second try\n",
    "# Hint: pd.crosstab and normalize may be helpful\n",
    "pd.crosstab(topic_wmeta_small.topics_clean, topic_wmeta_small.top_topic, normalize = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## D\n",
    "\n",
    "# Using a couple press releases as examples, write a 1-2 sentence interpretation of why some of \n",
    "# the manual topics map on more cleanly to an estimated topic than other manual topic(s)\n",
    "topic_wmeta.columns\n",
    "#topic_wmeta.head()\n",
    "topic_wmeta_clean = topic_wmeta.drop([\"doc_id\", \"date\", \"neg\", \"neu\", \"pos\", \"compound\", \"processed_text\"], \n",
    "                                     axis = 1).copy()\n",
    "\n",
    "# Example 1:\n",
    "topic_wmeta_clean[topic_wmeta_clean.topics_clean == \"Hate Crimes\"].sample(1)\n",
    "\n",
    "# Example 2: \n",
    "topic_wmeta_clean[topic_wmeta_clean.topics_clean == \"Civil Rights\"].sample(1)\n",
    "\n",
    "# Example 3: \n",
    "topic_wmeta_clean[topic_wmeta_clean.topics_clean == \"Project Safe Childhood\"].sample(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Response:** Although topic 1 corresponds well to \"Project Safe Childhood\", it is less precise to match the manual topics to topic_2 and topic 3 (see the top_topic values for topic Example 1 and Example 2 printed below). This is likely due to the overlap of the nature of hate crimes and civil rights crimes and that a civil rights division (components_clean) exists for the hate crime topic.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extend the analysis from unigrams to bigrams (9 points)\n",
    "\n",
    "In the previous question, you found top words via a unigram representation of the text. Now, we want to see how those top words change with bigrams (pairs of words)\n",
    "\n",
    "A. Using the `doj_subset_wscore` data and the `processed_text` column (so the words after stemming/other preprocessing), create a column in the data called `processed_text_bigrams` that combines each consecutive pairs of word into a bigram separated by an underscore. Eg:\n",
    "\n",
    "\"depart reach settlem\" would become \"depart_reach reach_settlem\"\n",
    "\n",
    "Do this by writing a function `create_bigram_onedoc` that takes in a single `processed_text` string and returns a string with its bigrams structured similarly to above example\n",
    " \n",
    "B. Print the `id`, `processed_text`, and `processed_text_bigram` columns for press release with id = 16-217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A \n",
    "# Using the doj_subset_wscore data and the processed_text column create a column in the data \n",
    "# called processed_text_bigrams that combines each consecutive pairs of word into a bigram separated by an underscore. Eg:\n",
    "# Do this by writing a function create_bigram_onedoc that takes in a single processed_text string and \n",
    "# returns a string with its bigrams structured similarly to above example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A\n",
    "\n",
    "# Step 1: define the function create_bigram_onedoc\n",
    "\n",
    "def create_bigram_onedoc(one_string):\n",
    "    '''Takes in a single processed_text string and returns a string with its bigrams structured \n",
    "    similarly so that that each consecutive pairs of word is combinedinto a bigram separated by an underscore\n",
    "    '''\n",
    "    split_list = one_string.split(\" \")   # split on space\n",
    "    bigrams = [str(first + \"_\" + second) for first, second in zip(split_list, split_list[1:])] # create bigram\n",
    "    return \" \".join(bigrams)   # rejoin into one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A \n",
    "\n",
    "# Step 2: run the function and save as another column\n",
    "doj_subset_wscore[\"processed_text_bigrams\"] = [create_bigram_onedoc(string) for string \n",
    "                                               in doj_subset_wscore.processed_text]\n",
    "\n",
    "# test this worked\n",
    "#doj_subset_wscore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B\n",
    "\n",
    "# Print the id, processed_text, and processed_text_bigram columns for press release with id = 16-217\n",
    "doj_subset_wscore[[\"id\", \"processed_text\", \"processed_text_bigrams\"]].loc[doj_subset_wscore.id == \"16-217\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Use the create_dtm function and the `processed_text_bigrams` column to create a document-term matrix (`dtm_bigram`) with these bigrams. Keep the following three columns in the data: `id`, `topics_clean`, and `compound` \n",
    "\n",
    "D. Print the \n",
    " (1) dimensions of the `dtm` matrix from question 2.2  and \n",
    " (2) the dimensions of the `dtm_bigram` matrix. Comment on why the bigram matrix has more dimensions than the unigram matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Find and print the 10 most prevelant bigrams for each of the three topics_clean using the `get_topwords` function from 2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## C\n",
    "\n",
    "# Use the create_dtm function and the processed_text_bigrams column to create a document-term matrix (dtm_bigram)\n",
    "# with these bigrams. Keep the following three columns in the data: id, topics_clean, and compound\n",
    "dtm_bigram = create_dtm(list_of_strings = doj_subset_wscore.processed_text_bigrams,\n",
    "                       metadata = doj_subset_wscore[[\"id\", \"topics_clean\", \"compound\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## D\n",
    "\n",
    "# Print the dimensions of the dtm matrix from question 2.2 \n",
    "doj_doc_matrix.shape\n",
    "\n",
    "# Print the dimensions of the dtm_bigram matrix. \n",
    "dtm_bigram.shape\n",
    "\n",
    "# Comment on why the bigram matrix has more dimensions than the unigram matrix\n",
    "# Response: The bigram matrix has more columns because in creating bigrams, we vastly increased the number of \n",
    "# potential tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## E \n",
    "\n",
    "# Find and print the 10 most prevelant bigrams for each of the three topics_clean using the get_topwords \n",
    "# function from 2.2\n",
    "hate_crimes_subset_b = dtm_bigram.loc[dtm_bigram.metadata_topics_clean == \"Hate Crimes\"]\n",
    "top_words(hate_crimes_subset_b)\n",
    "\n",
    "civil_rights_subset_b = dtm_bigram.loc[dtm_bigram.metadata_topics_clean == \"Civil Rights\"]\n",
    "top_words(civil_rights_subset_b)\n",
    "\n",
    "safe_child_subset_b = dtm_bigram.loc[dtm_bigram.metadata_topics_clean == \"Project Safe Childhood\"]\n",
    "top_words(safe_child_subset_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
